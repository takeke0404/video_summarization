{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_train",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv3bDuGqLEqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c8099a-0a8d-4fb9-e472-58c21fdbcc5d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqNSxcauckR2",
        "outputId": "0c94de9f-887c-4ab6-bfb9-82e8541ec59b"
      },
      "source": [
        "import tensorflow as tf\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.16.136.186:8470']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yp_XS0kLPzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcc125a-b0b2-4b96-d3ba-7ebfb80d2627"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install keras_bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
            "Requirement already satisfied: keras_bert in /usr/local/lib/python3.6/dist-packages (0.86.0)\n",
            "Requirement already satisfied: keras-transformer>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from keras_bert) (0.38.0)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras_bert) (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_bert) (1.18.5)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras_bert) (0.6.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras_bert) (0.14.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras_bert) (0.8.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.27.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras_bert) (0.27.0)\n",
            "Requirement already satisfied: keras-pos-embd>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras_bert) (0.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert) (2.10.0)\n",
            "Requirement already satisfied: keras-self-attention==0.46.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.38.0->keras_bert) (0.46.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras_bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79yl1-09LVDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f344905-0d0e-45df-9162-02523570a629"
      },
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "\n",
        "# feature.csvは上記で用意したファイルのパスを指定してください\n",
        "train_features_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/bert/data/trains/features.csv')\n",
        "\n",
        "def _get_indice(feature):\n",
        "    tokens = []\n",
        "    tokens.append('[CLS]')\n",
        "    tokens.extend(sp.encode_as_pieces(feature))\n",
        "    tokens.append('[SEP]')\n",
        "    number = len(tokens)\n",
        "\n",
        "    return number\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "# ダウンロードした事前学習モデルのパスを指定してください\n",
        "sp.Load('/content/drive/My Drive/Colab Notebooks/bert/bert-wiki-ja/wiki-ja.model')\n",
        "\n",
        "numbers = []\n",
        "\n",
        "for feature in train_features_df['feature']:\n",
        "    features_number = _get_indice(feature)\n",
        "    numbers.append(features_number)\n",
        "\n",
        "# 平均トークン数\n",
        "max_token_num = np.max(numbers)\n",
        "print(\"mean_token_number: \" + str(max_token_num))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_token_number: 1478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfwSnVnGL5Oz"
      },
      "source": [
        "import sys\n",
        "sys.path.append('modules')\n",
        "\n",
        "# BERTのロード\n",
        "config_path = '/content/drive/My Drive/Colab Notebooks/bert/bert-wiki-ja/bert_finetuning_config.json'\n",
        "# 拡張子まで記載しない\n",
        "checkpoint_path = '/content/drive/My Drive/Colab Notebooks/bert/bert-wiki-ja/model.ckpt-1400000'\n",
        "\n",
        "SEQ_LEN = 512\n",
        "BATCH_SIZE = 128\n",
        "BERT_DIM = 768\n",
        "LR = 1e-4\n",
        "# 学習回数\n",
        "EPOCH = 500\n",
        "\n",
        "maxlen = SEQ_LEN"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klvYEse-MjSA"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('/content/drive/My Drive/Colab Notebooks/bert/bert-wiki-ja/wiki-ja.model')\n",
        "\n",
        "def _get_indice(feature):\n",
        "    indices = np.zeros((maxlen), dtype = np.int32)\n",
        "\n",
        "    tokens = []\n",
        "    tokens.append('[CLS]')\n",
        "    tokens.extend(sp.encode_as_pieces(feature))\n",
        "    tokens.append('[SEP]')\n",
        "\n",
        "    for t, token in enumerate(tokens):\n",
        "        if t >= maxlen:\n",
        "            indices[0:maxlen-1]=indices[1:maxlen]\n",
        "            try:\n",
        "                indices[maxlen-1] = sp.piece_to_id(token)\n",
        "            except:\n",
        "                indices[maxlen-1] = sp.piece_to_id('<unk>')\n",
        "        else:\n",
        "            try:\n",
        "                indices[t] = sp.piece_to_id(token)\n",
        "            except:\n",
        "                indices[t] = sp.piece_to_id('<unk>')\n",
        "\n",
        "    return indices\n",
        "\n",
        "def _load_labeldata(train_dir, test_dir):\n",
        "    t = []\n",
        "    c = 0\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/bert/data/trains/features.csv') as f:\n",
        "      for text in csv.reader(f):\n",
        "        if(c!=0):\n",
        "          t.append(str(text))\n",
        "        c+=1\n",
        "    train_features_df = pd.DataFrame(t,columns=['feature'])\n",
        "    train_labels_df = pd.read_csv(f'{train_dir}/labels.csv',dtype = 'str')\n",
        "    t = []\n",
        "    c = 0\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/bert/data/tests/features.csv') as f:\n",
        "      for text in csv.reader(f):\n",
        "        if(c!=0):\n",
        "          t.append(str(text))\n",
        "        c+=1\n",
        "    test_features_df = pd.DataFrame(t,columns=['feature'])\n",
        "    test_labels_df = pd.read_csv(f'{test_dir}/labels.csv',dtype = 'str')\n",
        "    label2index = {k: i for i, k in enumerate(train_labels_df['label'].unique())}\n",
        "    index2label = {i: k for i, k in enumerate(train_labels_df['label'].unique())}\n",
        "    class_count = len(label2index)\n",
        "    train_labels = to_categorical([label2index[label] for label in train_labels_df['label']], num_classes=class_count)\n",
        "    test_label_indices = [label2index[label] for label in test_labels_df['label']]\n",
        "    test_labels = to_categorical(test_label_indices, num_classes=class_count)\n",
        "\n",
        "    train_features = []\n",
        "    test_features = []\n",
        "\n",
        "    for feature in train_features_df['feature']:\n",
        "        train_features.append(_get_indice(feature))\n",
        "    train_segments = np.zeros((len(train_features), maxlen), dtype = np.float32)\n",
        "    for feature in test_features_df['feature']:\n",
        "        test_features.append(_get_indice(feature))\n",
        "    test_segments = np.zeros((len(test_features), maxlen), dtype = np.float32)\n",
        "\n",
        "    print(f'Trainデータ数: {len(train_features_df)}, Testデータ数: {len(test_features_df)}, ラベル数: {class_count}')\n",
        "\n",
        "    return {\n",
        "        'class_count': class_count,\n",
        "        'label2index': label2index,\n",
        "        'index2label': index2label,\n",
        "        'train_labels': train_labels,\n",
        "        'test_labels': test_labels,\n",
        "        'test_label_indices': test_label_indices,\n",
        "        'train_features': np.array(train_features),\n",
        "        'train_segments': np.array(train_segments),\n",
        "        'test_features': np.array(test_features),\n",
        "        'test_segments': np.array(test_segments),\n",
        "        'input_len': maxlen\n",
        "    }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKB8mwM5Muuv"
      },
      "source": [
        "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Flatten, GlobalMaxPooling1D\n",
        "from keras_bert.layers import MaskedGlobalMaxPool1D\n",
        "from keras import Input, Model\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "\n",
        "def _create_model(input_shape, class_count):\n",
        "\n",
        "    bert = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True,  trainable=False, seq_len=SEQ_LEN)\n",
        "\n",
        "    bert_last = bert.get_layer(name='NSP-Dense').output\n",
        "    x1 = bert_last\n",
        "    output_tensor = Dense(class_count, activation='sigmoid')(x1)\n",
        " \n",
        "    model = Model([bert.input[0], bert.input[1]], output_tensor)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQvubfCZqtde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fee65c-4135-42bf-ec57-803c620e9d7a"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# TPUの各種セットアップ\n",
        "tpu_grpc_url = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"]\n",
        "tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
        "strategy = tf.distribute.TPUStrategy(tpu_cluster_resolver)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.16.136.186:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.16.136.186:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ffHM_lM0RM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283f2d71-9d57-4b78-faf2-e21989001161"
      },
      "source": [
        "# データロードとモデルの準備\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras_bert import AdamWarmup, calc_train_steps\n",
        "\n",
        "trains_dir = '/content/drive/My Drive/Colab Notebooks/bert/data/trains'\n",
        "tests_dir = '/content/drive/My Drive/Colab Notebooks/bert/data/tests'\n",
        "\n",
        "data = _load_labeldata(trains_dir, tests_dir)\n",
        "model_filename = '/content/drive/My Drive/Colab Notebooks/bert/models/youtube_finetuning.h5'\n",
        "\n",
        "decay_steps, warmup_steps = calc_train_steps(\n",
        "    data['train_features'].shape[0],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCH,\n",
        ")\n",
        "\n",
        "with strategy.scope():\n",
        "    model = _create_model(data['train_features'].shape, data['class_count'])\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR), \n",
        "                  metrics=['mae', 'mse', 'acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainデータ数: 5880, Testデータ数: 2395, ラベル数: 2\n",
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 512, 768), ( 24576000    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 512, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 512, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 512, 768)     393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 512, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 512, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 512, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 512, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 512, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            1538        NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 110,618,882\n",
            "Trainable params: 1,538\n",
            "Non-trainable params: 110,617,344\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MWym2UoTNgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813e3588-9c0e-4e36-8a9f-98529bcb6883"
      },
      "source": [
        "history = model.fit([data['train_features'], data['train_segments']],\n",
        "          data['train_labels'],\n",
        "          epochs = EPOCH,\n",
        "          batch_size = BATCH_SIZE,\n",
        "          validation_data=([data['test_features'], data['test_segments']], data['test_labels']),\n",
        "          shuffle=True,\n",
        "          verbose = 1)\n",
        "\n",
        "model.save_weights(model_filename)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 2/46 [>.............................] - ETA: 3:38 - loss: 0.9295 - mae: 0.5080 - mse: 0.3304 - acc: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.1343s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.1343s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - ETA: 0s - loss: 0.9445 - mae: 0.5137 - mse: 0.3367 - acc: 0.4507WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_test_batch_end` time: 0.1251s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_test_batch_end` time: 0.1251s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 44s 965ms/step - loss: 0.9445 - mae: 0.5137 - mse: 0.3367 - acc: 0.4507 - val_loss: 1.0328 - val_mae: 0.5446 - val_mse: 0.3683 - val_acc: 0.3495\n",
            "Epoch 2/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.9407 - mae: 0.5138 - mse: 0.3358 - acc: 0.4507 - val_loss: 1.0251 - val_mae: 0.5445 - val_mse: 0.3667 - val_acc: 0.3495\n",
            "Epoch 3/500\n",
            "46/46 [==============================] - 9s 197ms/step - loss: 0.9314 - mae: 0.5137 - mse: 0.3337 - acc: 0.4505 - val_loss: 1.0120 - val_mae: 0.5442 - val_mse: 0.3639 - val_acc: 0.3495\n",
            "Epoch 4/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.9212 - mae: 0.5142 - mse: 0.3315 - acc: 0.4502 - val_loss: 0.9947 - val_mae: 0.5435 - val_mse: 0.3600 - val_acc: 0.3495\n",
            "Epoch 5/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.9049 - mae: 0.5137 - mse: 0.3274 - acc: 0.4503 - val_loss: 0.9725 - val_mae: 0.5423 - val_mse: 0.3546 - val_acc: 0.3495\n",
            "Epoch 6/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.8849 - mae: 0.5132 - mse: 0.3223 - acc: 0.4503 - val_loss: 0.9466 - val_mae: 0.5405 - val_mse: 0.3478 - val_acc: 0.3495\n",
            "Epoch 7/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.8629 - mae: 0.5121 - mse: 0.3159 - acc: 0.4507 - val_loss: 0.9159 - val_mae: 0.5375 - val_mse: 0.3388 - val_acc: 0.3495\n",
            "Epoch 8/500\n",
            "46/46 [==============================] - 9s 198ms/step - loss: 0.8388 - mae: 0.5113 - mse: 0.3086 - acc: 0.4519 - val_loss: 0.8860 - val_mae: 0.5351 - val_mse: 0.3297 - val_acc: 0.3495\n",
            "Epoch 9/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.8146 - mae: 0.5096 - mse: 0.3005 - acc: 0.4515 - val_loss: 0.8528 - val_mae: 0.5307 - val_mse: 0.3183 - val_acc: 0.3486\n",
            "Epoch 10/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.7913 - mae: 0.5083 - mse: 0.2922 - acc: 0.4514 - val_loss: 0.8217 - val_mae: 0.5264 - val_mse: 0.3068 - val_acc: 0.3511\n",
            "Epoch 11/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.7689 - mae: 0.5071 - mse: 0.2838 - acc: 0.4568 - val_loss: 0.7919 - val_mae: 0.5214 - val_mse: 0.2949 - val_acc: 0.3587\n",
            "Epoch 12/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.7491 - mae: 0.5050 - mse: 0.2755 - acc: 0.4626 - val_loss: 0.7646 - val_mae: 0.5160 - val_mse: 0.2833 - val_acc: 0.3737\n",
            "Epoch 13/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.7320 - mae: 0.5034 - mse: 0.2682 - acc: 0.4689 - val_loss: 0.7411 - val_mae: 0.5106 - val_mse: 0.2728 - val_acc: 0.3883\n",
            "Epoch 14/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.7187 - mae: 0.5014 - mse: 0.2621 - acc: 0.4842 - val_loss: 0.7222 - val_mae: 0.5057 - val_mse: 0.2640 - val_acc: 0.4322\n",
            "Epoch 15/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.7084 - mae: 0.4999 - mse: 0.2574 - acc: 0.5044 - val_loss: 0.7065 - val_mae: 0.5008 - val_mse: 0.2565 - val_acc: 0.4747\n",
            "Epoch 16/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.7012 - mae: 0.4983 - mse: 0.2539 - acc: 0.5026 - val_loss: 0.6945 - val_mae: 0.4963 - val_mse: 0.2507 - val_acc: 0.5269\n",
            "Epoch 17/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6973 - mae: 0.4976 - mse: 0.2520 - acc: 0.5182 - val_loss: 0.6852 - val_mae: 0.4923 - val_mse: 0.2461 - val_acc: 0.5871\n",
            "Epoch 18/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6933 - mae: 0.4957 - mse: 0.2500 - acc: 0.5281 - val_loss: 0.6795 - val_mae: 0.4898 - val_mse: 0.2433 - val_acc: 0.6042\n",
            "Epoch 19/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6912 - mae: 0.4950 - mse: 0.2490 - acc: 0.5381 - val_loss: 0.6748 - val_mae: 0.4872 - val_mse: 0.2409 - val_acc: 0.6171\n",
            "Epoch 20/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6902 - mae: 0.4941 - mse: 0.2486 - acc: 0.5425 - val_loss: 0.6705 - val_mae: 0.4847 - val_mse: 0.2388 - val_acc: 0.6301\n",
            "Epoch 21/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6891 - mae: 0.4936 - mse: 0.2480 - acc: 0.5447 - val_loss: 0.6697 - val_mae: 0.4843 - val_mse: 0.2384 - val_acc: 0.6322\n",
            "Epoch 22/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6874 - mae: 0.4925 - mse: 0.2472 - acc: 0.5583 - val_loss: 0.6674 - val_mae: 0.4829 - val_mse: 0.2373 - val_acc: 0.6405\n",
            "Epoch 23/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6860 - mae: 0.4915 - mse: 0.2465 - acc: 0.5512 - val_loss: 0.6668 - val_mae: 0.4826 - val_mse: 0.2370 - val_acc: 0.6405\n",
            "Epoch 24/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6859 - mae: 0.4912 - mse: 0.2464 - acc: 0.5566 - val_loss: 0.6642 - val_mae: 0.4808 - val_mse: 0.2357 - val_acc: 0.6422\n",
            "Epoch 25/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6849 - mae: 0.4909 - mse: 0.2460 - acc: 0.5546 - val_loss: 0.6638 - val_mae: 0.4806 - val_mse: 0.2355 - val_acc: 0.6443\n",
            "Epoch 26/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6838 - mae: 0.4898 - mse: 0.2454 - acc: 0.5588 - val_loss: 0.6635 - val_mae: 0.4804 - val_mse: 0.2354 - val_acc: 0.6426\n",
            "Epoch 27/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6847 - mae: 0.4907 - mse: 0.2459 - acc: 0.5553 - val_loss: 0.6626 - val_mae: 0.4799 - val_mse: 0.2350 - val_acc: 0.6468\n",
            "Epoch 28/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.6844 - mae: 0.4906 - mse: 0.2457 - acc: 0.5544 - val_loss: 0.6622 - val_mae: 0.4797 - val_mse: 0.2347 - val_acc: 0.6505\n",
            "Epoch 29/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6831 - mae: 0.4898 - mse: 0.2451 - acc: 0.5531 - val_loss: 0.6594 - val_mae: 0.4777 - val_mse: 0.2334 - val_acc: 0.6505\n",
            "Epoch 30/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6819 - mae: 0.4887 - mse: 0.2445 - acc: 0.5616 - val_loss: 0.6608 - val_mae: 0.4789 - val_mse: 0.2341 - val_acc: 0.6559\n",
            "Epoch 31/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.6811 - mae: 0.4886 - mse: 0.2441 - acc: 0.5629 - val_loss: 0.6595 - val_mae: 0.4781 - val_mse: 0.2334 - val_acc: 0.6564\n",
            "Epoch 32/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6811 - mae: 0.4886 - mse: 0.2441 - acc: 0.5592 - val_loss: 0.6585 - val_mae: 0.4773 - val_mse: 0.2329 - val_acc: 0.6555\n",
            "Epoch 33/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6797 - mae: 0.4877 - mse: 0.2434 - acc: 0.5628 - val_loss: 0.6589 - val_mae: 0.4777 - val_mse: 0.2331 - val_acc: 0.6568\n",
            "Epoch 34/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.6786 - mae: 0.4870 - mse: 0.2429 - acc: 0.5689 - val_loss: 0.6576 - val_mae: 0.4768 - val_mse: 0.2325 - val_acc: 0.6572\n",
            "Epoch 35/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6795 - mae: 0.4877 - mse: 0.2433 - acc: 0.5622 - val_loss: 0.6603 - val_mae: 0.4789 - val_mse: 0.2338 - val_acc: 0.6597\n",
            "Epoch 36/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6780 - mae: 0.4871 - mse: 0.2426 - acc: 0.5733 - val_loss: 0.6550 - val_mae: 0.4750 - val_mse: 0.2312 - val_acc: 0.6597\n",
            "Epoch 37/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6780 - mae: 0.4866 - mse: 0.2426 - acc: 0.5709 - val_loss: 0.6540 - val_mae: 0.4744 - val_mse: 0.2308 - val_acc: 0.6622\n",
            "Epoch 38/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6767 - mae: 0.4857 - mse: 0.2420 - acc: 0.5707 - val_loss: 0.6556 - val_mae: 0.4756 - val_mse: 0.2315 - val_acc: 0.6639\n",
            "Epoch 39/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6753 - mae: 0.4851 - mse: 0.2413 - acc: 0.5808 - val_loss: 0.6540 - val_mae: 0.4745 - val_mse: 0.2308 - val_acc: 0.6639\n",
            "Epoch 40/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6755 - mae: 0.4855 - mse: 0.2414 - acc: 0.5776 - val_loss: 0.6519 - val_mae: 0.4729 - val_mse: 0.2298 - val_acc: 0.6656\n",
            "Epoch 41/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6743 - mae: 0.4843 - mse: 0.2408 - acc: 0.5816 - val_loss: 0.6534 - val_mae: 0.4741 - val_mse: 0.2305 - val_acc: 0.6610\n",
            "Epoch 42/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6742 - mae: 0.4846 - mse: 0.2408 - acc: 0.5821 - val_loss: 0.6512 - val_mae: 0.4723 - val_mse: 0.2294 - val_acc: 0.6660\n",
            "Epoch 43/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6739 - mae: 0.4838 - mse: 0.2406 - acc: 0.5821 - val_loss: 0.6527 - val_mae: 0.4736 - val_mse: 0.2302 - val_acc: 0.6526\n",
            "Epoch 44/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6730 - mae: 0.4834 - mse: 0.2402 - acc: 0.5828 - val_loss: 0.6526 - val_mae: 0.4736 - val_mse: 0.2301 - val_acc: 0.6505\n",
            "Epoch 45/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6719 - mae: 0.4832 - mse: 0.2396 - acc: 0.5847 - val_loss: 0.6482 - val_mae: 0.4701 - val_mse: 0.2280 - val_acc: 0.6681\n",
            "Epoch 46/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6714 - mae: 0.4822 - mse: 0.2394 - acc: 0.5852 - val_loss: 0.6501 - val_mae: 0.4718 - val_mse: 0.2289 - val_acc: 0.6576\n",
            "Epoch 47/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6706 - mae: 0.4817 - mse: 0.2390 - acc: 0.5861 - val_loss: 0.6493 - val_mae: 0.4711 - val_mse: 0.2285 - val_acc: 0.6539\n",
            "Epoch 48/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6707 - mae: 0.4817 - mse: 0.2390 - acc: 0.5895 - val_loss: 0.6490 - val_mae: 0.4708 - val_mse: 0.2284 - val_acc: 0.6514\n",
            "Epoch 49/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6705 - mae: 0.4819 - mse: 0.2389 - acc: 0.5861 - val_loss: 0.6452 - val_mae: 0.4676 - val_mse: 0.2265 - val_acc: 0.6635\n",
            "Epoch 50/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6687 - mae: 0.4803 - mse: 0.2381 - acc: 0.5913 - val_loss: 0.6459 - val_mae: 0.4684 - val_mse: 0.2269 - val_acc: 0.6551\n",
            "Epoch 51/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6691 - mae: 0.4802 - mse: 0.2383 - acc: 0.5861 - val_loss: 0.6477 - val_mae: 0.4698 - val_mse: 0.2277 - val_acc: 0.6514\n",
            "Epoch 52/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6675 - mae: 0.4799 - mse: 0.2375 - acc: 0.5930 - val_loss: 0.6462 - val_mae: 0.4686 - val_mse: 0.2271 - val_acc: 0.6526\n",
            "Epoch 53/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6677 - mae: 0.4800 - mse: 0.2376 - acc: 0.5906 - val_loss: 0.6440 - val_mae: 0.4668 - val_mse: 0.2260 - val_acc: 0.6555\n",
            "Epoch 54/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.6667 - mae: 0.4787 - mse: 0.2372 - acc: 0.5912 - val_loss: 0.6475 - val_mae: 0.4696 - val_mse: 0.2277 - val_acc: 0.6476\n",
            "Epoch 55/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6670 - mae: 0.4788 - mse: 0.2373 - acc: 0.5896 - val_loss: 0.6440 - val_mae: 0.4669 - val_mse: 0.2260 - val_acc: 0.6484\n",
            "Epoch 56/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6658 - mae: 0.4783 - mse: 0.2367 - acc: 0.5951 - val_loss: 0.6457 - val_mae: 0.4680 - val_mse: 0.2268 - val_acc: 0.6476\n",
            "Epoch 57/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6653 - mae: 0.4775 - mse: 0.2365 - acc: 0.5917 - val_loss: 0.6477 - val_mae: 0.4694 - val_mse: 0.2278 - val_acc: 0.6438\n",
            "Epoch 58/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6650 - mae: 0.4778 - mse: 0.2363 - acc: 0.5974 - val_loss: 0.6426 - val_mae: 0.4656 - val_mse: 0.2254 - val_acc: 0.6522\n",
            "Epoch 59/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6648 - mae: 0.4775 - mse: 0.2362 - acc: 0.5906 - val_loss: 0.6402 - val_mae: 0.4637 - val_mse: 0.2242 - val_acc: 0.6518\n",
            "Epoch 60/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6638 - mae: 0.4766 - mse: 0.2358 - acc: 0.5966 - val_loss: 0.6417 - val_mae: 0.4647 - val_mse: 0.2249 - val_acc: 0.6522\n",
            "Epoch 61/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6637 - mae: 0.4764 - mse: 0.2357 - acc: 0.5956 - val_loss: 0.6433 - val_mae: 0.4660 - val_mse: 0.2257 - val_acc: 0.6489\n",
            "Epoch 62/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6635 - mae: 0.4767 - mse: 0.2356 - acc: 0.5985 - val_loss: 0.6421 - val_mae: 0.4650 - val_mse: 0.2252 - val_acc: 0.6493\n",
            "Epoch 63/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6633 - mae: 0.4758 - mse: 0.2356 - acc: 0.5949 - val_loss: 0.6423 - val_mae: 0.4652 - val_mse: 0.2252 - val_acc: 0.6501\n",
            "Epoch 64/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6618 - mae: 0.4750 - mse: 0.2348 - acc: 0.5935 - val_loss: 0.6437 - val_mae: 0.4662 - val_mse: 0.2259 - val_acc: 0.6476\n",
            "Epoch 65/500\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 0.6629 - mae: 0.4757 - mse: 0.2354 - acc: 0.5973 - val_loss: 0.6416 - val_mae: 0.4645 - val_mse: 0.2249 - val_acc: 0.6497\n",
            "Epoch 66/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6609 - mae: 0.4742 - mse: 0.2344 - acc: 0.6014 - val_loss: 0.6415 - val_mae: 0.4643 - val_mse: 0.2249 - val_acc: 0.6522\n",
            "Epoch 67/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6622 - mae: 0.4754 - mse: 0.2350 - acc: 0.5981 - val_loss: 0.6414 - val_mae: 0.4641 - val_mse: 0.2248 - val_acc: 0.6509\n",
            "Epoch 68/500\n",
            "46/46 [==============================] - 9s 200ms/step - loss: 0.6602 - mae: 0.4733 - mse: 0.2340 - acc: 0.6026 - val_loss: 0.6417 - val_mae: 0.4644 - val_mse: 0.2250 - val_acc: 0.6514\n",
            "Epoch 69/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6632 - mae: 0.4751 - mse: 0.2355 - acc: 0.5978 - val_loss: 0.6423 - val_mae: 0.4647 - val_mse: 0.2253 - val_acc: 0.6451\n",
            "Epoch 70/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6620 - mae: 0.4747 - mse: 0.2349 - acc: 0.6010 - val_loss: 0.6405 - val_mae: 0.4633 - val_mse: 0.2244 - val_acc: 0.6522\n",
            "Epoch 71/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6604 - mae: 0.4738 - mse: 0.2342 - acc: 0.5988 - val_loss: 0.6396 - val_mae: 0.4625 - val_mse: 0.2240 - val_acc: 0.6522\n",
            "Epoch 72/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6601 - mae: 0.4727 - mse: 0.2340 - acc: 0.5986 - val_loss: 0.6425 - val_mae: 0.4645 - val_mse: 0.2254 - val_acc: 0.6430\n",
            "Epoch 73/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6589 - mae: 0.4728 - mse: 0.2335 - acc: 0.6053 - val_loss: 0.6400 - val_mae: 0.4627 - val_mse: 0.2242 - val_acc: 0.6497\n",
            "Epoch 74/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6598 - mae: 0.4732 - mse: 0.2339 - acc: 0.5995 - val_loss: 0.6372 - val_mae: 0.4605 - val_mse: 0.2229 - val_acc: 0.6530\n",
            "Epoch 75/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6601 - mae: 0.4724 - mse: 0.2341 - acc: 0.5957 - val_loss: 0.6387 - val_mae: 0.4616 - val_mse: 0.2236 - val_acc: 0.6526\n",
            "Epoch 76/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6597 - mae: 0.4726 - mse: 0.2339 - acc: 0.5998 - val_loss: 0.6388 - val_mae: 0.4617 - val_mse: 0.2236 - val_acc: 0.6501\n",
            "Epoch 77/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6599 - mae: 0.4728 - mse: 0.2339 - acc: 0.6009 - val_loss: 0.6375 - val_mae: 0.4605 - val_mse: 0.2230 - val_acc: 0.6526\n",
            "Epoch 78/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6604 - mae: 0.4728 - mse: 0.2342 - acc: 0.6019 - val_loss: 0.6377 - val_mae: 0.4605 - val_mse: 0.2232 - val_acc: 0.6526\n",
            "Epoch 79/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6581 - mae: 0.4714 - mse: 0.2331 - acc: 0.6043 - val_loss: 0.6388 - val_mae: 0.4615 - val_mse: 0.2237 - val_acc: 0.6480\n",
            "Epoch 80/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6573 - mae: 0.4713 - mse: 0.2327 - acc: 0.6029 - val_loss: 0.6382 - val_mae: 0.4609 - val_mse: 0.2234 - val_acc: 0.6514\n",
            "Epoch 81/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6590 - mae: 0.4716 - mse: 0.2335 - acc: 0.5990 - val_loss: 0.6362 - val_mae: 0.4594 - val_mse: 0.2224 - val_acc: 0.6534\n",
            "Epoch 82/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6583 - mae: 0.4713 - mse: 0.2332 - acc: 0.5939 - val_loss: 0.6371 - val_mae: 0.4599 - val_mse: 0.2229 - val_acc: 0.6489\n",
            "Epoch 83/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6579 - mae: 0.4708 - mse: 0.2331 - acc: 0.6010 - val_loss: 0.6391 - val_mae: 0.4614 - val_mse: 0.2238 - val_acc: 0.6472\n",
            "Epoch 84/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6562 - mae: 0.4705 - mse: 0.2322 - acc: 0.6105 - val_loss: 0.6355 - val_mae: 0.4585 - val_mse: 0.2221 - val_acc: 0.6530\n",
            "Epoch 85/500\n",
            "46/46 [==============================] - 10s 214ms/step - loss: 0.6566 - mae: 0.4696 - mse: 0.2324 - acc: 0.6019 - val_loss: 0.6388 - val_mae: 0.4611 - val_mse: 0.2237 - val_acc: 0.6480\n",
            "Epoch 86/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6590 - mae: 0.4716 - mse: 0.2336 - acc: 0.5966 - val_loss: 0.6378 - val_mae: 0.4601 - val_mse: 0.2232 - val_acc: 0.6480\n",
            "Epoch 87/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6557 - mae: 0.4700 - mse: 0.2321 - acc: 0.6022 - val_loss: 0.6364 - val_mae: 0.4590 - val_mse: 0.2226 - val_acc: 0.6509\n",
            "Epoch 88/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6569 - mae: 0.4693 - mse: 0.2326 - acc: 0.6000 - val_loss: 0.6379 - val_mae: 0.4602 - val_mse: 0.2233 - val_acc: 0.6459\n",
            "Epoch 89/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6543 - mae: 0.4686 - mse: 0.2313 - acc: 0.6102 - val_loss: 0.6396 - val_mae: 0.4614 - val_mse: 0.2241 - val_acc: 0.6443\n",
            "Epoch 90/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6577 - mae: 0.4704 - mse: 0.2330 - acc: 0.6031 - val_loss: 0.6343 - val_mae: 0.4571 - val_mse: 0.2216 - val_acc: 0.6526\n",
            "Epoch 91/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6577 - mae: 0.4697 - mse: 0.2329 - acc: 0.5980 - val_loss: 0.6383 - val_mae: 0.4603 - val_mse: 0.2235 - val_acc: 0.6443\n",
            "Epoch 92/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6552 - mae: 0.4690 - mse: 0.2317 - acc: 0.6063 - val_loss: 0.6355 - val_mae: 0.4582 - val_mse: 0.2221 - val_acc: 0.6501\n",
            "Epoch 93/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6560 - mae: 0.4690 - mse: 0.2320 - acc: 0.6122 - val_loss: 0.6356 - val_mae: 0.4581 - val_mse: 0.2222 - val_acc: 0.6501\n",
            "Epoch 94/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6550 - mae: 0.4688 - mse: 0.2317 - acc: 0.6078 - val_loss: 0.6333 - val_mae: 0.4563 - val_mse: 0.2212 - val_acc: 0.6526\n",
            "Epoch 95/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6545 - mae: 0.4676 - mse: 0.2315 - acc: 0.6031 - val_loss: 0.6371 - val_mae: 0.4593 - val_mse: 0.2229 - val_acc: 0.6447\n",
            "Epoch 96/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6554 - mae: 0.4687 - mse: 0.2319 - acc: 0.6117 - val_loss: 0.6346 - val_mae: 0.4574 - val_mse: 0.2217 - val_acc: 0.6505\n",
            "Epoch 97/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6567 - mae: 0.4692 - mse: 0.2325 - acc: 0.6102 - val_loss: 0.6330 - val_mae: 0.4559 - val_mse: 0.2210 - val_acc: 0.6534\n",
            "Epoch 98/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6551 - mae: 0.4687 - mse: 0.2317 - acc: 0.6048 - val_loss: 0.6333 - val_mae: 0.4562 - val_mse: 0.2211 - val_acc: 0.6526\n",
            "Epoch 99/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6560 - mae: 0.4683 - mse: 0.2322 - acc: 0.6043 - val_loss: 0.6349 - val_mae: 0.4574 - val_mse: 0.2219 - val_acc: 0.6493\n",
            "Epoch 100/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6532 - mae: 0.4672 - mse: 0.2309 - acc: 0.6097 - val_loss: 0.6335 - val_mae: 0.4563 - val_mse: 0.2213 - val_acc: 0.6526\n",
            "Epoch 101/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6539 - mae: 0.4675 - mse: 0.2311 - acc: 0.6061 - val_loss: 0.6356 - val_mae: 0.4579 - val_mse: 0.2222 - val_acc: 0.6459\n",
            "Epoch 102/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6541 - mae: 0.4675 - mse: 0.2313 - acc: 0.6048 - val_loss: 0.6314 - val_mae: 0.4543 - val_mse: 0.2203 - val_acc: 0.6543\n",
            "Epoch 103/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6552 - mae: 0.4671 - mse: 0.2317 - acc: 0.6075 - val_loss: 0.6346 - val_mae: 0.4570 - val_mse: 0.2218 - val_acc: 0.6493\n",
            "Epoch 104/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6540 - mae: 0.4678 - mse: 0.2312 - acc: 0.6107 - val_loss: 0.6330 - val_mae: 0.4556 - val_mse: 0.2210 - val_acc: 0.6547\n",
            "Epoch 105/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6533 - mae: 0.4665 - mse: 0.2309 - acc: 0.6071 - val_loss: 0.6375 - val_mae: 0.4589 - val_mse: 0.2232 - val_acc: 0.6426\n",
            "Epoch 106/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6544 - mae: 0.4674 - mse: 0.2314 - acc: 0.6083 - val_loss: 0.6342 - val_mae: 0.4565 - val_mse: 0.2216 - val_acc: 0.6497\n",
            "Epoch 107/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6539 - mae: 0.4670 - mse: 0.2311 - acc: 0.6090 - val_loss: 0.6366 - val_mae: 0.4583 - val_mse: 0.2227 - val_acc: 0.6438\n",
            "Epoch 108/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6515 - mae: 0.4656 - mse: 0.2300 - acc: 0.6134 - val_loss: 0.6345 - val_mae: 0.4566 - val_mse: 0.2217 - val_acc: 0.6472\n",
            "Epoch 109/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6545 - mae: 0.4670 - mse: 0.2315 - acc: 0.6077 - val_loss: 0.6360 - val_mae: 0.4576 - val_mse: 0.2225 - val_acc: 0.6455\n",
            "Epoch 110/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6538 - mae: 0.4669 - mse: 0.2312 - acc: 0.6075 - val_loss: 0.6346 - val_mae: 0.4566 - val_mse: 0.2218 - val_acc: 0.6447\n",
            "Epoch 111/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6533 - mae: 0.4668 - mse: 0.2310 - acc: 0.6092 - val_loss: 0.6317 - val_mae: 0.4543 - val_mse: 0.2205 - val_acc: 0.6543\n",
            "Epoch 112/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6543 - mae: 0.4666 - mse: 0.2314 - acc: 0.6085 - val_loss: 0.6365 - val_mae: 0.4579 - val_mse: 0.2227 - val_acc: 0.6418\n",
            "Epoch 113/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6536 - mae: 0.4665 - mse: 0.2310 - acc: 0.6085 - val_loss: 0.6345 - val_mae: 0.4564 - val_mse: 0.2218 - val_acc: 0.6493\n",
            "Epoch 114/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6515 - mae: 0.4657 - mse: 0.2300 - acc: 0.6139 - val_loss: 0.6352 - val_mae: 0.4569 - val_mse: 0.2221 - val_acc: 0.6463\n",
            "Epoch 115/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6530 - mae: 0.4656 - mse: 0.2307 - acc: 0.6107 - val_loss: 0.6334 - val_mae: 0.4553 - val_mse: 0.2213 - val_acc: 0.6484\n",
            "Epoch 116/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6531 - mae: 0.4659 - mse: 0.2308 - acc: 0.6085 - val_loss: 0.6345 - val_mae: 0.4563 - val_mse: 0.2218 - val_acc: 0.6484\n",
            "Epoch 117/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6511 - mae: 0.4655 - mse: 0.2298 - acc: 0.6167 - val_loss: 0.6291 - val_mae: 0.4518 - val_mse: 0.2193 - val_acc: 0.6564\n",
            "Epoch 118/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6527 - mae: 0.4651 - mse: 0.2306 - acc: 0.6075 - val_loss: 0.6352 - val_mae: 0.4567 - val_mse: 0.2221 - val_acc: 0.6443\n",
            "Epoch 119/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6510 - mae: 0.4649 - mse: 0.2299 - acc: 0.6121 - val_loss: 0.6345 - val_mae: 0.4562 - val_mse: 0.2218 - val_acc: 0.6459\n",
            "Epoch 120/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6540 - mae: 0.4664 - mse: 0.2313 - acc: 0.6058 - val_loss: 0.6351 - val_mae: 0.4566 - val_mse: 0.2221 - val_acc: 0.6438\n",
            "Epoch 121/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6532 - mae: 0.4658 - mse: 0.2309 - acc: 0.6121 - val_loss: 0.6319 - val_mae: 0.4541 - val_mse: 0.2206 - val_acc: 0.6518\n",
            "Epoch 122/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6502 - mae: 0.4643 - mse: 0.2294 - acc: 0.6160 - val_loss: 0.6335 - val_mae: 0.4552 - val_mse: 0.2213 - val_acc: 0.6505\n",
            "Epoch 123/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6532 - mae: 0.4656 - mse: 0.2309 - acc: 0.6054 - val_loss: 0.6329 - val_mae: 0.4548 - val_mse: 0.2210 - val_acc: 0.6497\n",
            "Epoch 124/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6540 - mae: 0.4662 - mse: 0.2312 - acc: 0.6100 - val_loss: 0.6328 - val_mae: 0.4546 - val_mse: 0.2210 - val_acc: 0.6493\n",
            "Epoch 125/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6522 - mae: 0.4647 - mse: 0.2306 - acc: 0.6102 - val_loss: 0.6346 - val_mae: 0.4561 - val_mse: 0.2219 - val_acc: 0.6459\n",
            "Epoch 126/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6516 - mae: 0.4648 - mse: 0.2301 - acc: 0.6111 - val_loss: 0.6337 - val_mae: 0.4554 - val_mse: 0.2214 - val_acc: 0.6451\n",
            "Epoch 127/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6502 - mae: 0.4641 - mse: 0.2295 - acc: 0.6077 - val_loss: 0.6340 - val_mae: 0.4556 - val_mse: 0.2216 - val_acc: 0.6455\n",
            "Epoch 128/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6493 - mae: 0.4634 - mse: 0.2290 - acc: 0.6245 - val_loss: 0.6320 - val_mae: 0.4541 - val_mse: 0.2206 - val_acc: 0.6518\n",
            "Epoch 129/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6522 - mae: 0.4647 - mse: 0.2304 - acc: 0.6117 - val_loss: 0.6345 - val_mae: 0.4558 - val_mse: 0.2218 - val_acc: 0.6451\n",
            "Epoch 130/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6511 - mae: 0.4645 - mse: 0.2299 - acc: 0.6107 - val_loss: 0.6314 - val_mae: 0.4534 - val_mse: 0.2204 - val_acc: 0.6509\n",
            "Epoch 131/500\n",
            "46/46 [==============================] - 10s 215ms/step - loss: 0.6535 - mae: 0.4655 - mse: 0.2310 - acc: 0.6100 - val_loss: 0.6329 - val_mae: 0.4546 - val_mse: 0.2211 - val_acc: 0.6468\n",
            "Epoch 132/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6531 - mae: 0.4650 - mse: 0.2308 - acc: 0.6056 - val_loss: 0.6312 - val_mae: 0.4533 - val_mse: 0.2203 - val_acc: 0.6505\n",
            "Epoch 133/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6503 - mae: 0.4638 - mse: 0.2296 - acc: 0.6122 - val_loss: 0.6345 - val_mae: 0.4558 - val_mse: 0.2219 - val_acc: 0.6476\n",
            "Epoch 134/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6512 - mae: 0.4640 - mse: 0.2300 - acc: 0.6102 - val_loss: 0.6314 - val_mae: 0.4534 - val_mse: 0.2204 - val_acc: 0.6505\n",
            "Epoch 135/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6516 - mae: 0.4643 - mse: 0.2301 - acc: 0.6141 - val_loss: 0.6325 - val_mae: 0.4541 - val_mse: 0.2209 - val_acc: 0.6459\n",
            "Epoch 136/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6524 - mae: 0.4644 - mse: 0.2306 - acc: 0.6061 - val_loss: 0.6316 - val_mae: 0.4535 - val_mse: 0.2205 - val_acc: 0.6505\n",
            "Epoch 137/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6509 - mae: 0.4638 - mse: 0.2299 - acc: 0.6078 - val_loss: 0.6340 - val_mae: 0.4551 - val_mse: 0.2216 - val_acc: 0.6463\n",
            "Epoch 138/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6539 - mae: 0.4658 - mse: 0.2312 - acc: 0.6073 - val_loss: 0.6335 - val_mae: 0.4549 - val_mse: 0.2214 - val_acc: 0.6459\n",
            "Epoch 139/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6508 - mae: 0.4639 - mse: 0.2297 - acc: 0.6116 - val_loss: 0.6307 - val_mae: 0.4528 - val_mse: 0.2201 - val_acc: 0.6493\n",
            "Epoch 140/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6504 - mae: 0.4640 - mse: 0.2296 - acc: 0.6078 - val_loss: 0.6271 - val_mae: 0.4496 - val_mse: 0.2184 - val_acc: 0.6605\n",
            "Epoch 141/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6502 - mae: 0.4628 - mse: 0.2295 - acc: 0.6105 - val_loss: 0.6305 - val_mae: 0.4524 - val_mse: 0.2200 - val_acc: 0.6505\n",
            "Epoch 142/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6503 - mae: 0.4638 - mse: 0.2295 - acc: 0.6153 - val_loss: 0.6289 - val_mae: 0.4512 - val_mse: 0.2192 - val_acc: 0.6534\n",
            "Epoch 143/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6487 - mae: 0.4627 - mse: 0.2288 - acc: 0.6128 - val_loss: 0.6295 - val_mae: 0.4515 - val_mse: 0.2195 - val_acc: 0.6522\n",
            "Epoch 144/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6484 - mae: 0.4619 - mse: 0.2286 - acc: 0.6165 - val_loss: 0.6323 - val_mae: 0.4539 - val_mse: 0.2208 - val_acc: 0.6476\n",
            "Epoch 145/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6508 - mae: 0.4634 - mse: 0.2299 - acc: 0.6043 - val_loss: 0.6318 - val_mae: 0.4533 - val_mse: 0.2206 - val_acc: 0.6472\n",
            "Epoch 146/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6475 - mae: 0.4617 - mse: 0.2282 - acc: 0.6185 - val_loss: 0.6336 - val_mae: 0.4546 - val_mse: 0.2214 - val_acc: 0.6472\n",
            "Epoch 147/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6486 - mae: 0.4630 - mse: 0.2287 - acc: 0.6163 - val_loss: 0.6301 - val_mae: 0.4520 - val_mse: 0.2198 - val_acc: 0.6463\n",
            "Epoch 148/500\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 0.6508 - mae: 0.4624 - mse: 0.2298 - acc: 0.6083 - val_loss: 0.6332 - val_mae: 0.4543 - val_mse: 0.2212 - val_acc: 0.6480\n",
            "Epoch 149/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6488 - mae: 0.4624 - mse: 0.2288 - acc: 0.6172 - val_loss: 0.6317 - val_mae: 0.4530 - val_mse: 0.2206 - val_acc: 0.6476\n",
            "Epoch 150/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6499 - mae: 0.4631 - mse: 0.2294 - acc: 0.6151 - val_loss: 0.6285 - val_mae: 0.4506 - val_mse: 0.2191 - val_acc: 0.6551\n",
            "Epoch 151/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6482 - mae: 0.4615 - mse: 0.2286 - acc: 0.6153 - val_loss: 0.6330 - val_mae: 0.4541 - val_mse: 0.2212 - val_acc: 0.6497\n",
            "Epoch 152/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6501 - mae: 0.4628 - mse: 0.2295 - acc: 0.6150 - val_loss: 0.6333 - val_mae: 0.4541 - val_mse: 0.2214 - val_acc: 0.6501\n",
            "Epoch 153/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6498 - mae: 0.4628 - mse: 0.2293 - acc: 0.6068 - val_loss: 0.6319 - val_mae: 0.4531 - val_mse: 0.2207 - val_acc: 0.6480\n",
            "Epoch 154/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6491 - mae: 0.4621 - mse: 0.2291 - acc: 0.6094 - val_loss: 0.6297 - val_mae: 0.4515 - val_mse: 0.2196 - val_acc: 0.6468\n",
            "Epoch 155/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6515 - mae: 0.4634 - mse: 0.2301 - acc: 0.6105 - val_loss: 0.6306 - val_mae: 0.4523 - val_mse: 0.2200 - val_acc: 0.6476\n",
            "Epoch 156/500\n",
            "46/46 [==============================] - 9s 207ms/step - loss: 0.6498 - mae: 0.4627 - mse: 0.2293 - acc: 0.6160 - val_loss: 0.6285 - val_mae: 0.4505 - val_mse: 0.2191 - val_acc: 0.6539\n",
            "Epoch 157/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6493 - mae: 0.4622 - mse: 0.2291 - acc: 0.6097 - val_loss: 0.6296 - val_mae: 0.4515 - val_mse: 0.2196 - val_acc: 0.6509\n",
            "Epoch 158/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6496 - mae: 0.4623 - mse: 0.2292 - acc: 0.6163 - val_loss: 0.6313 - val_mae: 0.4527 - val_mse: 0.2204 - val_acc: 0.6468\n",
            "Epoch 159/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6487 - mae: 0.4622 - mse: 0.2288 - acc: 0.6126 - val_loss: 0.6307 - val_mae: 0.4522 - val_mse: 0.2201 - val_acc: 0.6463\n",
            "Epoch 160/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6510 - mae: 0.4630 - mse: 0.2298 - acc: 0.6184 - val_loss: 0.6291 - val_mae: 0.4509 - val_mse: 0.2194 - val_acc: 0.6505\n",
            "Epoch 161/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6480 - mae: 0.4607 - mse: 0.2285 - acc: 0.6129 - val_loss: 0.6358 - val_mae: 0.4558 - val_mse: 0.2225 - val_acc: 0.6434\n",
            "Epoch 162/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6503 - mae: 0.4628 - mse: 0.2296 - acc: 0.6185 - val_loss: 0.6330 - val_mae: 0.4539 - val_mse: 0.2212 - val_acc: 0.6484\n",
            "Epoch 163/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6504 - mae: 0.4628 - mse: 0.2296 - acc: 0.6088 - val_loss: 0.6306 - val_mae: 0.4520 - val_mse: 0.2201 - val_acc: 0.6451\n",
            "Epoch 164/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6498 - mae: 0.4620 - mse: 0.2294 - acc: 0.6117 - val_loss: 0.6298 - val_mae: 0.4514 - val_mse: 0.2197 - val_acc: 0.6480\n",
            "Epoch 165/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6481 - mae: 0.4617 - mse: 0.2285 - acc: 0.6192 - val_loss: 0.6304 - val_mae: 0.4518 - val_mse: 0.2200 - val_acc: 0.6468\n",
            "Epoch 166/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6495 - mae: 0.4620 - mse: 0.2292 - acc: 0.6114 - val_loss: 0.6287 - val_mae: 0.4506 - val_mse: 0.2191 - val_acc: 0.6534\n",
            "Epoch 167/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6494 - mae: 0.4614 - mse: 0.2293 - acc: 0.6129 - val_loss: 0.6304 - val_mae: 0.4518 - val_mse: 0.2200 - val_acc: 0.6476\n",
            "Epoch 168/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6483 - mae: 0.4616 - mse: 0.2286 - acc: 0.6167 - val_loss: 0.6310 - val_mae: 0.4523 - val_mse: 0.2203 - val_acc: 0.6480\n",
            "Epoch 169/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6487 - mae: 0.4615 - mse: 0.2289 - acc: 0.6124 - val_loss: 0.6315 - val_mae: 0.4526 - val_mse: 0.2205 - val_acc: 0.6489\n",
            "Epoch 170/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6461 - mae: 0.4605 - mse: 0.2276 - acc: 0.6206 - val_loss: 0.6288 - val_mae: 0.4503 - val_mse: 0.2192 - val_acc: 0.6514\n",
            "Epoch 171/500\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.6488 - mae: 0.4616 - mse: 0.2289 - acc: 0.6185 - val_loss: 0.6295 - val_mae: 0.4510 - val_mse: 0.2196 - val_acc: 0.6501\n",
            "Epoch 172/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6488 - mae: 0.4615 - mse: 0.2289 - acc: 0.6192 - val_loss: 0.6302 - val_mae: 0.4517 - val_mse: 0.2199 - val_acc: 0.6493\n",
            "Epoch 173/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6472 - mae: 0.4612 - mse: 0.2282 - acc: 0.6214 - val_loss: 0.6282 - val_mae: 0.4498 - val_mse: 0.2190 - val_acc: 0.6526\n",
            "Epoch 174/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6489 - mae: 0.4616 - mse: 0.2289 - acc: 0.6133 - val_loss: 0.6268 - val_mae: 0.4484 - val_mse: 0.2183 - val_acc: 0.6543\n",
            "Epoch 175/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6489 - mae: 0.4608 - mse: 0.2288 - acc: 0.6143 - val_loss: 0.6314 - val_mae: 0.4523 - val_mse: 0.2205 - val_acc: 0.6489\n",
            "Epoch 176/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6477 - mae: 0.4613 - mse: 0.2283 - acc: 0.6148 - val_loss: 0.6299 - val_mae: 0.4513 - val_mse: 0.2198 - val_acc: 0.6493\n",
            "Epoch 177/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6485 - mae: 0.4605 - mse: 0.2287 - acc: 0.6138 - val_loss: 0.6306 - val_mae: 0.4517 - val_mse: 0.2201 - val_acc: 0.6476\n",
            "Epoch 178/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6513 - mae: 0.4629 - mse: 0.2301 - acc: 0.6066 - val_loss: 0.6329 - val_mae: 0.4534 - val_mse: 0.2212 - val_acc: 0.6468\n",
            "Epoch 179/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6479 - mae: 0.4612 - mse: 0.2285 - acc: 0.6180 - val_loss: 0.6288 - val_mae: 0.4503 - val_mse: 0.2192 - val_acc: 0.6526\n",
            "Epoch 180/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6491 - mae: 0.4610 - mse: 0.2290 - acc: 0.6128 - val_loss: 0.6330 - val_mae: 0.4535 - val_mse: 0.2212 - val_acc: 0.6476\n",
            "Epoch 181/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6491 - mae: 0.4617 - mse: 0.2290 - acc: 0.6221 - val_loss: 0.6288 - val_mae: 0.4504 - val_mse: 0.2193 - val_acc: 0.6518\n",
            "Epoch 182/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6466 - mae: 0.4603 - mse: 0.2279 - acc: 0.6204 - val_loss: 0.6305 - val_mae: 0.4516 - val_mse: 0.2200 - val_acc: 0.6484\n",
            "Epoch 183/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6491 - mae: 0.4613 - mse: 0.2290 - acc: 0.6121 - val_loss: 0.6275 - val_mae: 0.4490 - val_mse: 0.2187 - val_acc: 0.6526\n",
            "Epoch 184/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6508 - mae: 0.4622 - mse: 0.2299 - acc: 0.6131 - val_loss: 0.6299 - val_mae: 0.4510 - val_mse: 0.2198 - val_acc: 0.6489\n",
            "Epoch 185/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6471 - mae: 0.4605 - mse: 0.2282 - acc: 0.6162 - val_loss: 0.6313 - val_mae: 0.4521 - val_mse: 0.2204 - val_acc: 0.6505\n",
            "Epoch 186/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6484 - mae: 0.4616 - mse: 0.2287 - acc: 0.6189 - val_loss: 0.6300 - val_mae: 0.4511 - val_mse: 0.2198 - val_acc: 0.6493\n",
            "Epoch 187/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6475 - mae: 0.4604 - mse: 0.2282 - acc: 0.6226 - val_loss: 0.6296 - val_mae: 0.4507 - val_mse: 0.2196 - val_acc: 0.6489\n",
            "Epoch 188/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6462 - mae: 0.4603 - mse: 0.2277 - acc: 0.6194 - val_loss: 0.6299 - val_mae: 0.4510 - val_mse: 0.2198 - val_acc: 0.6505\n",
            "Epoch 189/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6492 - mae: 0.4617 - mse: 0.2291 - acc: 0.6139 - val_loss: 0.6288 - val_mae: 0.4502 - val_mse: 0.2193 - val_acc: 0.6509\n",
            "Epoch 190/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6502 - mae: 0.4615 - mse: 0.2296 - acc: 0.6075 - val_loss: 0.6304 - val_mae: 0.4514 - val_mse: 0.2200 - val_acc: 0.6509\n",
            "Epoch 191/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6481 - mae: 0.4608 - mse: 0.2286 - acc: 0.6165 - val_loss: 0.6327 - val_mae: 0.4532 - val_mse: 0.2211 - val_acc: 0.6463\n",
            "Epoch 192/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6460 - mae: 0.4603 - mse: 0.2277 - acc: 0.6170 - val_loss: 0.6298 - val_mae: 0.4511 - val_mse: 0.2197 - val_acc: 0.6534\n",
            "Epoch 193/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6474 - mae: 0.4599 - mse: 0.2283 - acc: 0.6114 - val_loss: 0.6322 - val_mae: 0.4527 - val_mse: 0.2209 - val_acc: 0.6472\n",
            "Epoch 194/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6468 - mae: 0.4600 - mse: 0.2280 - acc: 0.6179 - val_loss: 0.6317 - val_mae: 0.4523 - val_mse: 0.2206 - val_acc: 0.6472\n",
            "Epoch 195/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6482 - mae: 0.4611 - mse: 0.2286 - acc: 0.6146 - val_loss: 0.6300 - val_mae: 0.4510 - val_mse: 0.2198 - val_acc: 0.6501\n",
            "Epoch 196/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6447 - mae: 0.4589 - mse: 0.2270 - acc: 0.6194 - val_loss: 0.6307 - val_mae: 0.4515 - val_mse: 0.2202 - val_acc: 0.6489\n",
            "Epoch 197/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6468 - mae: 0.4599 - mse: 0.2280 - acc: 0.6138 - val_loss: 0.6304 - val_mae: 0.4513 - val_mse: 0.2200 - val_acc: 0.6509\n",
            "Epoch 198/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6472 - mae: 0.4605 - mse: 0.2280 - acc: 0.6131 - val_loss: 0.6301 - val_mae: 0.4510 - val_mse: 0.2199 - val_acc: 0.6497\n",
            "Epoch 199/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6474 - mae: 0.4604 - mse: 0.2281 - acc: 0.6228 - val_loss: 0.6300 - val_mae: 0.4509 - val_mse: 0.2199 - val_acc: 0.6501\n",
            "Epoch 200/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6480 - mae: 0.4604 - mse: 0.2285 - acc: 0.6173 - val_loss: 0.6292 - val_mae: 0.4503 - val_mse: 0.2195 - val_acc: 0.6505\n",
            "Epoch 201/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6478 - mae: 0.4601 - mse: 0.2285 - acc: 0.6160 - val_loss: 0.6283 - val_mae: 0.4495 - val_mse: 0.2190 - val_acc: 0.6543\n",
            "Epoch 202/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6466 - mae: 0.4605 - mse: 0.2279 - acc: 0.6168 - val_loss: 0.6314 - val_mae: 0.4519 - val_mse: 0.2205 - val_acc: 0.6480\n",
            "Epoch 203/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6446 - mae: 0.4592 - mse: 0.2269 - acc: 0.6182 - val_loss: 0.6271 - val_mae: 0.4484 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 204/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6480 - mae: 0.4598 - mse: 0.2285 - acc: 0.6158 - val_loss: 0.6333 - val_mae: 0.4533 - val_mse: 0.2214 - val_acc: 0.6455\n",
            "Epoch 205/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6458 - mae: 0.4602 - mse: 0.2274 - acc: 0.6230 - val_loss: 0.6269 - val_mae: 0.4484 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 206/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6488 - mae: 0.4604 - mse: 0.2289 - acc: 0.6170 - val_loss: 0.6281 - val_mae: 0.4494 - val_mse: 0.2190 - val_acc: 0.6530\n",
            "Epoch 207/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6475 - mae: 0.4602 - mse: 0.2283 - acc: 0.6133 - val_loss: 0.6292 - val_mae: 0.4502 - val_mse: 0.2195 - val_acc: 0.6534\n",
            "Epoch 208/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6474 - mae: 0.4602 - mse: 0.2283 - acc: 0.6094 - val_loss: 0.6299 - val_mae: 0.4508 - val_mse: 0.2198 - val_acc: 0.6514\n",
            "Epoch 209/500\n",
            "46/46 [==============================] - 10s 217ms/step - loss: 0.6448 - mae: 0.4587 - mse: 0.2268 - acc: 0.6281 - val_loss: 0.6296 - val_mae: 0.4505 - val_mse: 0.2197 - val_acc: 0.6514\n",
            "Epoch 210/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6467 - mae: 0.4601 - mse: 0.2279 - acc: 0.6189 - val_loss: 0.6279 - val_mae: 0.4491 - val_mse: 0.2189 - val_acc: 0.6518\n",
            "Epoch 211/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6456 - mae: 0.4588 - mse: 0.2275 - acc: 0.6168 - val_loss: 0.6305 - val_mae: 0.4512 - val_mse: 0.2201 - val_acc: 0.6497\n",
            "Epoch 212/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6471 - mae: 0.4604 - mse: 0.2281 - acc: 0.6151 - val_loss: 0.6291 - val_mae: 0.4501 - val_mse: 0.2194 - val_acc: 0.6509\n",
            "Epoch 213/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6486 - mae: 0.4609 - mse: 0.2288 - acc: 0.6150 - val_loss: 0.6287 - val_mae: 0.4496 - val_mse: 0.2193 - val_acc: 0.6505\n",
            "Epoch 214/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6475 - mae: 0.4599 - mse: 0.2284 - acc: 0.6187 - val_loss: 0.6294 - val_mae: 0.4501 - val_mse: 0.2196 - val_acc: 0.6493\n",
            "Epoch 215/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6471 - mae: 0.4601 - mse: 0.2282 - acc: 0.6192 - val_loss: 0.6290 - val_mae: 0.4499 - val_mse: 0.2194 - val_acc: 0.6514\n",
            "Epoch 216/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6468 - mae: 0.4595 - mse: 0.2280 - acc: 0.6136 - val_loss: 0.6299 - val_mae: 0.4505 - val_mse: 0.2198 - val_acc: 0.6505\n",
            "Epoch 217/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6476 - mae: 0.4602 - mse: 0.2284 - acc: 0.6100 - val_loss: 0.6290 - val_mae: 0.4499 - val_mse: 0.2194 - val_acc: 0.6505\n",
            "Epoch 218/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6458 - mae: 0.4593 - mse: 0.2275 - acc: 0.6129 - val_loss: 0.6298 - val_mae: 0.4505 - val_mse: 0.2198 - val_acc: 0.6514\n",
            "Epoch 219/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6466 - mae: 0.4596 - mse: 0.2279 - acc: 0.6156 - val_loss: 0.6285 - val_mae: 0.4496 - val_mse: 0.2192 - val_acc: 0.6526\n",
            "Epoch 220/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6447 - mae: 0.4587 - mse: 0.2270 - acc: 0.6207 - val_loss: 0.6298 - val_mae: 0.4505 - val_mse: 0.2198 - val_acc: 0.6522\n",
            "Epoch 221/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6482 - mae: 0.4602 - mse: 0.2286 - acc: 0.6190 - val_loss: 0.6285 - val_mae: 0.4494 - val_mse: 0.2192 - val_acc: 0.6522\n",
            "Epoch 222/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6466 - mae: 0.4595 - mse: 0.2279 - acc: 0.6162 - val_loss: 0.6305 - val_mae: 0.4510 - val_mse: 0.2201 - val_acc: 0.6501\n",
            "Epoch 223/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6440 - mae: 0.4582 - mse: 0.2267 - acc: 0.6281 - val_loss: 0.6301 - val_mae: 0.4506 - val_mse: 0.2199 - val_acc: 0.6493\n",
            "Epoch 224/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6468 - mae: 0.4597 - mse: 0.2279 - acc: 0.6216 - val_loss: 0.6290 - val_mae: 0.4498 - val_mse: 0.2194 - val_acc: 0.6505\n",
            "Epoch 225/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6477 - mae: 0.4596 - mse: 0.2285 - acc: 0.6090 - val_loss: 0.6313 - val_mae: 0.4515 - val_mse: 0.2205 - val_acc: 0.6484\n",
            "Epoch 226/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6482 - mae: 0.4604 - mse: 0.2286 - acc: 0.6163 - val_loss: 0.6306 - val_mae: 0.4511 - val_mse: 0.2201 - val_acc: 0.6518\n",
            "Epoch 227/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6456 - mae: 0.4588 - mse: 0.2275 - acc: 0.6136 - val_loss: 0.6277 - val_mae: 0.4487 - val_mse: 0.2188 - val_acc: 0.6509\n",
            "Epoch 228/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6466 - mae: 0.4596 - mse: 0.2278 - acc: 0.6179 - val_loss: 0.6291 - val_mae: 0.4500 - val_mse: 0.2195 - val_acc: 0.6526\n",
            "Epoch 229/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6463 - mae: 0.4596 - mse: 0.2278 - acc: 0.6194 - val_loss: 0.6274 - val_mae: 0.4485 - val_mse: 0.2186 - val_acc: 0.6505\n",
            "Epoch 230/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6475 - mae: 0.4597 - mse: 0.2283 - acc: 0.6151 - val_loss: 0.6282 - val_mae: 0.4492 - val_mse: 0.2190 - val_acc: 0.6509\n",
            "Epoch 231/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6449 - mae: 0.4587 - mse: 0.2271 - acc: 0.6243 - val_loss: 0.6278 - val_mae: 0.4489 - val_mse: 0.2189 - val_acc: 0.6505\n",
            "Epoch 232/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6467 - mae: 0.4590 - mse: 0.2279 - acc: 0.6211 - val_loss: 0.6284 - val_mae: 0.4494 - val_mse: 0.2191 - val_acc: 0.6505\n",
            "Epoch 233/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6454 - mae: 0.4590 - mse: 0.2274 - acc: 0.6126 - val_loss: 0.6280 - val_mae: 0.4490 - val_mse: 0.2190 - val_acc: 0.6509\n",
            "Epoch 234/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6457 - mae: 0.4588 - mse: 0.2275 - acc: 0.6156 - val_loss: 0.6299 - val_mae: 0.4505 - val_mse: 0.2199 - val_acc: 0.6522\n",
            "Epoch 235/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6470 - mae: 0.4599 - mse: 0.2280 - acc: 0.6252 - val_loss: 0.6286 - val_mae: 0.4494 - val_mse: 0.2193 - val_acc: 0.6509\n",
            "Epoch 236/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6454 - mae: 0.4589 - mse: 0.2274 - acc: 0.6194 - val_loss: 0.6309 - val_mae: 0.4512 - val_mse: 0.2203 - val_acc: 0.6505\n",
            "Epoch 237/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6436 - mae: 0.4586 - mse: 0.2265 - acc: 0.6228 - val_loss: 0.6278 - val_mae: 0.4488 - val_mse: 0.2189 - val_acc: 0.6509\n",
            "Epoch 238/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6471 - mae: 0.4592 - mse: 0.2282 - acc: 0.6182 - val_loss: 0.6275 - val_mae: 0.4485 - val_mse: 0.2187 - val_acc: 0.6497\n",
            "Epoch 239/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6454 - mae: 0.4586 - mse: 0.2272 - acc: 0.6163 - val_loss: 0.6308 - val_mae: 0.4511 - val_mse: 0.2203 - val_acc: 0.6514\n",
            "Epoch 240/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6465 - mae: 0.4594 - mse: 0.2279 - acc: 0.6180 - val_loss: 0.6278 - val_mae: 0.4488 - val_mse: 0.2189 - val_acc: 0.6518\n",
            "Epoch 241/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6445 - mae: 0.4576 - mse: 0.2270 - acc: 0.6213 - val_loss: 0.6299 - val_mae: 0.4504 - val_mse: 0.2198 - val_acc: 0.6509\n",
            "Epoch 242/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6457 - mae: 0.4592 - mse: 0.2274 - acc: 0.6192 - val_loss: 0.6296 - val_mae: 0.4503 - val_mse: 0.2197 - val_acc: 0.6509\n",
            "Epoch 243/500\n",
            "46/46 [==============================] - 10s 216ms/step - loss: 0.6475 - mae: 0.4597 - mse: 0.2283 - acc: 0.6114 - val_loss: 0.6279 - val_mae: 0.4489 - val_mse: 0.2189 - val_acc: 0.6514\n",
            "Epoch 244/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6467 - mae: 0.4588 - mse: 0.2280 - acc: 0.6184 - val_loss: 0.6282 - val_mae: 0.4492 - val_mse: 0.2191 - val_acc: 0.6509\n",
            "Epoch 245/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6445 - mae: 0.4588 - mse: 0.2269 - acc: 0.6116 - val_loss: 0.6291 - val_mae: 0.4498 - val_mse: 0.2195 - val_acc: 0.6522\n",
            "Epoch 246/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6463 - mae: 0.4589 - mse: 0.2278 - acc: 0.6213 - val_loss: 0.6277 - val_mae: 0.4488 - val_mse: 0.2188 - val_acc: 0.6522\n",
            "Epoch 247/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6448 - mae: 0.4586 - mse: 0.2271 - acc: 0.6207 - val_loss: 0.6280 - val_mae: 0.4489 - val_mse: 0.2189 - val_acc: 0.6526\n",
            "Epoch 248/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6457 - mae: 0.4586 - mse: 0.2275 - acc: 0.6214 - val_loss: 0.6286 - val_mae: 0.4495 - val_mse: 0.2192 - val_acc: 0.6539\n",
            "Epoch 249/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6466 - mae: 0.4593 - mse: 0.2279 - acc: 0.6155 - val_loss: 0.6301 - val_mae: 0.4506 - val_mse: 0.2199 - val_acc: 0.6543\n",
            "Epoch 250/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6452 - mae: 0.4590 - mse: 0.2273 - acc: 0.6219 - val_loss: 0.6272 - val_mae: 0.4483 - val_mse: 0.2186 - val_acc: 0.6509\n",
            "Epoch 251/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6471 - mae: 0.4589 - mse: 0.2282 - acc: 0.6156 - val_loss: 0.6285 - val_mae: 0.4493 - val_mse: 0.2192 - val_acc: 0.6530\n",
            "Epoch 252/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6442 - mae: 0.4582 - mse: 0.2267 - acc: 0.6209 - val_loss: 0.6281 - val_mae: 0.4490 - val_mse: 0.2190 - val_acc: 0.6530\n",
            "Epoch 253/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6464 - mae: 0.4588 - mse: 0.2278 - acc: 0.6136 - val_loss: 0.6270 - val_mae: 0.4482 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 254/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6487 - mae: 0.4598 - mse: 0.2289 - acc: 0.6175 - val_loss: 0.6289 - val_mae: 0.4496 - val_mse: 0.2194 - val_acc: 0.6539\n",
            "Epoch 255/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6436 - mae: 0.4581 - mse: 0.2265 - acc: 0.6247 - val_loss: 0.6281 - val_mae: 0.4489 - val_mse: 0.2190 - val_acc: 0.6522\n",
            "Epoch 256/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6435 - mae: 0.4576 - mse: 0.2265 - acc: 0.6279 - val_loss: 0.6283 - val_mae: 0.4491 - val_mse: 0.2191 - val_acc: 0.6530\n",
            "Epoch 257/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6476 - mae: 0.4593 - mse: 0.2285 - acc: 0.6145 - val_loss: 0.6296 - val_mae: 0.4501 - val_mse: 0.2197 - val_acc: 0.6526\n",
            "Epoch 258/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6479 - mae: 0.4601 - mse: 0.2285 - acc: 0.6138 - val_loss: 0.6277 - val_mae: 0.4486 - val_mse: 0.2188 - val_acc: 0.6518\n",
            "Epoch 259/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6450 - mae: 0.4584 - mse: 0.2272 - acc: 0.6199 - val_loss: 0.6269 - val_mae: 0.4480 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 260/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6469 - mae: 0.4590 - mse: 0.2280 - acc: 0.6201 - val_loss: 0.6280 - val_mae: 0.4489 - val_mse: 0.2190 - val_acc: 0.6514\n",
            "Epoch 261/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6455 - mae: 0.4588 - mse: 0.2273 - acc: 0.6209 - val_loss: 0.6263 - val_mae: 0.4475 - val_mse: 0.2182 - val_acc: 0.6522\n",
            "Epoch 262/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6455 - mae: 0.4583 - mse: 0.2274 - acc: 0.6248 - val_loss: 0.6272 - val_mae: 0.4483 - val_mse: 0.2186 - val_acc: 0.6522\n",
            "Epoch 263/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6468 - mae: 0.4594 - mse: 0.2280 - acc: 0.6121 - val_loss: 0.6276 - val_mae: 0.4485 - val_mse: 0.2188 - val_acc: 0.6514\n",
            "Epoch 264/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6441 - mae: 0.4576 - mse: 0.2266 - acc: 0.6231 - val_loss: 0.6268 - val_mae: 0.4479 - val_mse: 0.2184 - val_acc: 0.6522\n",
            "Epoch 265/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6446 - mae: 0.4576 - mse: 0.2271 - acc: 0.6182 - val_loss: 0.6286 - val_mae: 0.4493 - val_mse: 0.2192 - val_acc: 0.6534\n",
            "Epoch 266/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6428 - mae: 0.4574 - mse: 0.2262 - acc: 0.6243 - val_loss: 0.6296 - val_mae: 0.4500 - val_mse: 0.2197 - val_acc: 0.6522\n",
            "Epoch 267/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6460 - mae: 0.4587 - mse: 0.2277 - acc: 0.6146 - val_loss: 0.6276 - val_mae: 0.4484 - val_mse: 0.2188 - val_acc: 0.6501\n",
            "Epoch 268/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6429 - mae: 0.4569 - mse: 0.2261 - acc: 0.6219 - val_loss: 0.6285 - val_mae: 0.4492 - val_mse: 0.2192 - val_acc: 0.6539\n",
            "Epoch 269/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6461 - mae: 0.4588 - mse: 0.2276 - acc: 0.6259 - val_loss: 0.6279 - val_mae: 0.4488 - val_mse: 0.2189 - val_acc: 0.6539\n",
            "Epoch 270/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6471 - mae: 0.4592 - mse: 0.2282 - acc: 0.6158 - val_loss: 0.6286 - val_mae: 0.4494 - val_mse: 0.2192 - val_acc: 0.6534\n",
            "Epoch 271/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6462 - mae: 0.4592 - mse: 0.2278 - acc: 0.6148 - val_loss: 0.6264 - val_mae: 0.4475 - val_mse: 0.2182 - val_acc: 0.6518\n",
            "Epoch 272/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6426 - mae: 0.4566 - mse: 0.2260 - acc: 0.6250 - val_loss: 0.6279 - val_mae: 0.4487 - val_mse: 0.2189 - val_acc: 0.6530\n",
            "Epoch 273/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6463 - mae: 0.4586 - mse: 0.2277 - acc: 0.6204 - val_loss: 0.6280 - val_mae: 0.4487 - val_mse: 0.2190 - val_acc: 0.6534\n",
            "Epoch 274/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6429 - mae: 0.4570 - mse: 0.2262 - acc: 0.6265 - val_loss: 0.6304 - val_mae: 0.4505 - val_mse: 0.2201 - val_acc: 0.6514\n",
            "Epoch 275/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6441 - mae: 0.4579 - mse: 0.2267 - acc: 0.6182 - val_loss: 0.6271 - val_mae: 0.4479 - val_mse: 0.2186 - val_acc: 0.6501\n",
            "Epoch 276/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6468 - mae: 0.4588 - mse: 0.2280 - acc: 0.6179 - val_loss: 0.6284 - val_mae: 0.4490 - val_mse: 0.2192 - val_acc: 0.6543\n",
            "Epoch 277/500\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.6458 - mae: 0.4587 - mse: 0.2275 - acc: 0.6167 - val_loss: 0.6291 - val_mae: 0.4495 - val_mse: 0.2195 - val_acc: 0.6539\n",
            "Epoch 278/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6424 - mae: 0.4569 - mse: 0.2259 - acc: 0.6255 - val_loss: 0.6273 - val_mae: 0.4481 - val_mse: 0.2187 - val_acc: 0.6497\n",
            "Epoch 279/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6452 - mae: 0.4579 - mse: 0.2272 - acc: 0.6213 - val_loss: 0.6262 - val_mae: 0.4473 - val_mse: 0.2182 - val_acc: 0.6539\n",
            "Epoch 280/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6456 - mae: 0.4586 - mse: 0.2274 - acc: 0.6182 - val_loss: 0.6270 - val_mae: 0.4478 - val_mse: 0.2185 - val_acc: 0.6501\n",
            "Epoch 281/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6472 - mae: 0.4589 - mse: 0.2282 - acc: 0.6139 - val_loss: 0.6268 - val_mae: 0.4476 - val_mse: 0.2184 - val_acc: 0.6505\n",
            "Epoch 282/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6464 - mae: 0.4583 - mse: 0.2278 - acc: 0.6206 - val_loss: 0.6279 - val_mae: 0.4487 - val_mse: 0.2189 - val_acc: 0.6526\n",
            "Epoch 283/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6455 - mae: 0.4584 - mse: 0.2275 - acc: 0.6148 - val_loss: 0.6287 - val_mae: 0.4493 - val_mse: 0.2193 - val_acc: 0.6543\n",
            "Epoch 284/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6451 - mae: 0.4575 - mse: 0.2272 - acc: 0.6255 - val_loss: 0.6271 - val_mae: 0.4480 - val_mse: 0.2186 - val_acc: 0.6509\n",
            "Epoch 285/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6441 - mae: 0.4578 - mse: 0.2267 - acc: 0.6233 - val_loss: 0.6270 - val_mae: 0.4479 - val_mse: 0.2185 - val_acc: 0.6518\n",
            "Epoch 286/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6450 - mae: 0.4577 - mse: 0.2272 - acc: 0.6190 - val_loss: 0.6294 - val_mae: 0.4498 - val_mse: 0.2196 - val_acc: 0.6530\n",
            "Epoch 287/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6454 - mae: 0.4582 - mse: 0.2274 - acc: 0.6138 - val_loss: 0.6292 - val_mae: 0.4496 - val_mse: 0.2195 - val_acc: 0.6543\n",
            "Epoch 288/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6457 - mae: 0.4587 - mse: 0.2274 - acc: 0.6224 - val_loss: 0.6284 - val_mae: 0.4489 - val_mse: 0.2192 - val_acc: 0.6539\n",
            "Epoch 289/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6458 - mae: 0.4582 - mse: 0.2276 - acc: 0.6184 - val_loss: 0.6278 - val_mae: 0.4486 - val_mse: 0.2189 - val_acc: 0.6539\n",
            "Epoch 290/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6444 - mae: 0.4577 - mse: 0.2269 - acc: 0.6206 - val_loss: 0.6287 - val_mae: 0.4493 - val_mse: 0.2193 - val_acc: 0.6543\n",
            "Epoch 291/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6420 - mae: 0.4568 - mse: 0.2258 - acc: 0.6247 - val_loss: 0.6266 - val_mae: 0.4477 - val_mse: 0.2184 - val_acc: 0.6539\n",
            "Epoch 292/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6435 - mae: 0.4568 - mse: 0.2264 - acc: 0.6211 - val_loss: 0.6286 - val_mae: 0.4492 - val_mse: 0.2193 - val_acc: 0.6547\n",
            "Epoch 293/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6459 - mae: 0.4585 - mse: 0.2277 - acc: 0.6133 - val_loss: 0.6281 - val_mae: 0.4488 - val_mse: 0.2191 - val_acc: 0.6530\n",
            "Epoch 294/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6457 - mae: 0.4585 - mse: 0.2275 - acc: 0.6180 - val_loss: 0.6271 - val_mae: 0.4480 - val_mse: 0.2186 - val_acc: 0.6534\n",
            "Epoch 295/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6427 - mae: 0.4563 - mse: 0.2261 - acc: 0.6150 - val_loss: 0.6270 - val_mae: 0.4480 - val_mse: 0.2185 - val_acc: 0.6522\n",
            "Epoch 296/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6423 - mae: 0.4571 - mse: 0.2259 - acc: 0.6216 - val_loss: 0.6277 - val_mae: 0.4485 - val_mse: 0.2188 - val_acc: 0.6526\n",
            "Epoch 297/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6447 - mae: 0.4573 - mse: 0.2270 - acc: 0.6201 - val_loss: 0.6276 - val_mae: 0.4484 - val_mse: 0.2188 - val_acc: 0.6530\n",
            "Epoch 298/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6444 - mae: 0.4575 - mse: 0.2268 - acc: 0.6260 - val_loss: 0.6284 - val_mae: 0.4490 - val_mse: 0.2192 - val_acc: 0.6530\n",
            "Epoch 299/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6463 - mae: 0.4584 - mse: 0.2278 - acc: 0.6146 - val_loss: 0.6273 - val_mae: 0.4482 - val_mse: 0.2187 - val_acc: 0.6522\n",
            "Epoch 300/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6431 - mae: 0.4570 - mse: 0.2263 - acc: 0.6272 - val_loss: 0.6270 - val_mae: 0.4479 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 301/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6460 - mae: 0.4588 - mse: 0.2277 - acc: 0.6172 - val_loss: 0.6281 - val_mae: 0.4488 - val_mse: 0.2191 - val_acc: 0.6526\n",
            "Epoch 302/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6467 - mae: 0.4587 - mse: 0.2281 - acc: 0.6121 - val_loss: 0.6266 - val_mae: 0.4476 - val_mse: 0.2183 - val_acc: 0.6539\n",
            "Epoch 303/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6454 - mae: 0.4579 - mse: 0.2273 - acc: 0.6177 - val_loss: 0.6268 - val_mae: 0.4478 - val_mse: 0.2184 - val_acc: 0.6526\n",
            "Epoch 304/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6438 - mae: 0.4571 - mse: 0.2266 - acc: 0.6243 - val_loss: 0.6267 - val_mae: 0.4477 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 305/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6447 - mae: 0.4573 - mse: 0.2271 - acc: 0.6201 - val_loss: 0.6268 - val_mae: 0.4477 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 306/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6434 - mae: 0.4569 - mse: 0.2263 - acc: 0.6269 - val_loss: 0.6287 - val_mae: 0.4492 - val_mse: 0.2193 - val_acc: 0.6543\n",
            "Epoch 307/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6447 - mae: 0.4577 - mse: 0.2271 - acc: 0.6235 - val_loss: 0.6277 - val_mae: 0.4485 - val_mse: 0.2189 - val_acc: 0.6526\n",
            "Epoch 308/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6450 - mae: 0.4579 - mse: 0.2273 - acc: 0.6202 - val_loss: 0.6257 - val_mae: 0.4469 - val_mse: 0.2180 - val_acc: 0.6547\n",
            "Epoch 309/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6423 - mae: 0.4567 - mse: 0.2260 - acc: 0.6235 - val_loss: 0.6278 - val_mae: 0.4485 - val_mse: 0.2189 - val_acc: 0.6539\n",
            "Epoch 310/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6467 - mae: 0.4582 - mse: 0.2280 - acc: 0.6121 - val_loss: 0.6272 - val_mae: 0.4480 - val_mse: 0.2186 - val_acc: 0.6534\n",
            "Epoch 311/500\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.6441 - mae: 0.4578 - mse: 0.2269 - acc: 0.6182 - val_loss: 0.6278 - val_mae: 0.4485 - val_mse: 0.2189 - val_acc: 0.6539\n",
            "Epoch 312/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6456 - mae: 0.4586 - mse: 0.2274 - acc: 0.6189 - val_loss: 0.6273 - val_mae: 0.4482 - val_mse: 0.2187 - val_acc: 0.6526\n",
            "Epoch 313/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6444 - mae: 0.4573 - mse: 0.2268 - acc: 0.6265 - val_loss: 0.6285 - val_mae: 0.4491 - val_mse: 0.2192 - val_acc: 0.6530\n",
            "Epoch 314/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6438 - mae: 0.4572 - mse: 0.2267 - acc: 0.6209 - val_loss: 0.6274 - val_mae: 0.4482 - val_mse: 0.2187 - val_acc: 0.6514\n",
            "Epoch 315/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6426 - mae: 0.4560 - mse: 0.2260 - acc: 0.6259 - val_loss: 0.6280 - val_mae: 0.4487 - val_mse: 0.2190 - val_acc: 0.6522\n",
            "Epoch 316/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6445 - mae: 0.4572 - mse: 0.2269 - acc: 0.6194 - val_loss: 0.6266 - val_mae: 0.4476 - val_mse: 0.2183 - val_acc: 0.6539\n",
            "Epoch 317/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6413 - mae: 0.4561 - mse: 0.2255 - acc: 0.6245 - val_loss: 0.6282 - val_mae: 0.4488 - val_mse: 0.2191 - val_acc: 0.6534\n",
            "Epoch 318/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6444 - mae: 0.4578 - mse: 0.2269 - acc: 0.6262 - val_loss: 0.6267 - val_mae: 0.4476 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 319/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6454 - mae: 0.4580 - mse: 0.2274 - acc: 0.6165 - val_loss: 0.6275 - val_mae: 0.4482 - val_mse: 0.2188 - val_acc: 0.6509\n",
            "Epoch 320/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6444 - mae: 0.4569 - mse: 0.2269 - acc: 0.6247 - val_loss: 0.6278 - val_mae: 0.4485 - val_mse: 0.2189 - val_acc: 0.6534\n",
            "Epoch 321/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6454 - mae: 0.4577 - mse: 0.2273 - acc: 0.6252 - val_loss: 0.6279 - val_mae: 0.4485 - val_mse: 0.2190 - val_acc: 0.6530\n",
            "Epoch 322/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6447 - mae: 0.4580 - mse: 0.2269 - acc: 0.6221 - val_loss: 0.6271 - val_mae: 0.4479 - val_mse: 0.2186 - val_acc: 0.6514\n",
            "Epoch 323/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6417 - mae: 0.4557 - mse: 0.2256 - acc: 0.6228 - val_loss: 0.6270 - val_mae: 0.4478 - val_mse: 0.2186 - val_acc: 0.6514\n",
            "Epoch 324/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6436 - mae: 0.4570 - mse: 0.2266 - acc: 0.6197 - val_loss: 0.6275 - val_mae: 0.4482 - val_mse: 0.2188 - val_acc: 0.6518\n",
            "Epoch 325/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6445 - mae: 0.4576 - mse: 0.2270 - acc: 0.6192 - val_loss: 0.6274 - val_mae: 0.4482 - val_mse: 0.2187 - val_acc: 0.6522\n",
            "Epoch 326/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6450 - mae: 0.4581 - mse: 0.2272 - acc: 0.6182 - val_loss: 0.6271 - val_mae: 0.4479 - val_mse: 0.2186 - val_acc: 0.6518\n",
            "Epoch 327/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6448 - mae: 0.4574 - mse: 0.2271 - acc: 0.6160 - val_loss: 0.6266 - val_mae: 0.4475 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 328/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6437 - mae: 0.4569 - mse: 0.2266 - acc: 0.6204 - val_loss: 0.6277 - val_mae: 0.4484 - val_mse: 0.2189 - val_acc: 0.6526\n",
            "Epoch 329/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6443 - mae: 0.4577 - mse: 0.2269 - acc: 0.6194 - val_loss: 0.6290 - val_mae: 0.4494 - val_mse: 0.2195 - val_acc: 0.6509\n",
            "Epoch 330/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6459 - mae: 0.4578 - mse: 0.2277 - acc: 0.6224 - val_loss: 0.6276 - val_mae: 0.4483 - val_mse: 0.2188 - val_acc: 0.6518\n",
            "Epoch 331/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6430 - mae: 0.4568 - mse: 0.2262 - acc: 0.6231 - val_loss: 0.6275 - val_mae: 0.4481 - val_mse: 0.2188 - val_acc: 0.6526\n",
            "Epoch 332/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6440 - mae: 0.4572 - mse: 0.2267 - acc: 0.6155 - val_loss: 0.6283 - val_mae: 0.4487 - val_mse: 0.2192 - val_acc: 0.6518\n",
            "Epoch 333/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6438 - mae: 0.4570 - mse: 0.2266 - acc: 0.6243 - val_loss: 0.6268 - val_mae: 0.4476 - val_mse: 0.2185 - val_acc: 0.6522\n",
            "Epoch 334/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6426 - mae: 0.4566 - mse: 0.2261 - acc: 0.6231 - val_loss: 0.6274 - val_mae: 0.4481 - val_mse: 0.2187 - val_acc: 0.6526\n",
            "Epoch 335/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6432 - mae: 0.4567 - mse: 0.2263 - acc: 0.6194 - val_loss: 0.6275 - val_mae: 0.4482 - val_mse: 0.2188 - val_acc: 0.6522\n",
            "Epoch 336/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6436 - mae: 0.4571 - mse: 0.2266 - acc: 0.6196 - val_loss: 0.6278 - val_mae: 0.4483 - val_mse: 0.2189 - val_acc: 0.6539\n",
            "Epoch 337/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6444 - mae: 0.4570 - mse: 0.2268 - acc: 0.6177 - val_loss: 0.6261 - val_mae: 0.4470 - val_mse: 0.2181 - val_acc: 0.6559\n",
            "Epoch 338/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6433 - mae: 0.4564 - mse: 0.2264 - acc: 0.6196 - val_loss: 0.6278 - val_mae: 0.4484 - val_mse: 0.2189 - val_acc: 0.6514\n",
            "Epoch 339/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6448 - mae: 0.4573 - mse: 0.2271 - acc: 0.6204 - val_loss: 0.6268 - val_mae: 0.4476 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 340/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6447 - mae: 0.4578 - mse: 0.2271 - acc: 0.6202 - val_loss: 0.6287 - val_mae: 0.4491 - val_mse: 0.2193 - val_acc: 0.6522\n",
            "Epoch 341/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6429 - mae: 0.4566 - mse: 0.2263 - acc: 0.6223 - val_loss: 0.6278 - val_mae: 0.4483 - val_mse: 0.2189 - val_acc: 0.6522\n",
            "Epoch 342/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6435 - mae: 0.4567 - mse: 0.2265 - acc: 0.6224 - val_loss: 0.6267 - val_mae: 0.4475 - val_mse: 0.2184 - val_acc: 0.6547\n",
            "Epoch 343/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6421 - mae: 0.4562 - mse: 0.2258 - acc: 0.6276 - val_loss: 0.6270 - val_mae: 0.4478 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 344/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6399 - mae: 0.4550 - mse: 0.2249 - acc: 0.6250 - val_loss: 0.6272 - val_mae: 0.4479 - val_mse: 0.2187 - val_acc: 0.6526\n",
            "Epoch 345/500\n",
            "46/46 [==============================] - 10s 216ms/step - loss: 0.6442 - mae: 0.4574 - mse: 0.2268 - acc: 0.6226 - val_loss: 0.6285 - val_mae: 0.4489 - val_mse: 0.2193 - val_acc: 0.6514\n",
            "Epoch 346/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6434 - mae: 0.4571 - mse: 0.2264 - acc: 0.6253 - val_loss: 0.6271 - val_mae: 0.4477 - val_mse: 0.2186 - val_acc: 0.6547\n",
            "Epoch 347/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6433 - mae: 0.4571 - mse: 0.2263 - acc: 0.6265 - val_loss: 0.6271 - val_mae: 0.4477 - val_mse: 0.2186 - val_acc: 0.6534\n",
            "Epoch 348/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6431 - mae: 0.4565 - mse: 0.2263 - acc: 0.6172 - val_loss: 0.6277 - val_mae: 0.4482 - val_mse: 0.2189 - val_acc: 0.6518\n",
            "Epoch 349/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6450 - mae: 0.4579 - mse: 0.2272 - acc: 0.6201 - val_loss: 0.6279 - val_mae: 0.4483 - val_mse: 0.2190 - val_acc: 0.6526\n",
            "Epoch 350/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6432 - mae: 0.4567 - mse: 0.2263 - acc: 0.6221 - val_loss: 0.6276 - val_mae: 0.4481 - val_mse: 0.2189 - val_acc: 0.6526\n",
            "Epoch 351/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6459 - mae: 0.4580 - mse: 0.2275 - acc: 0.6167 - val_loss: 0.6284 - val_mae: 0.4487 - val_mse: 0.2192 - val_acc: 0.6505\n",
            "Epoch 352/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6461 - mae: 0.4581 - mse: 0.2278 - acc: 0.6133 - val_loss: 0.6273 - val_mae: 0.4479 - val_mse: 0.2187 - val_acc: 0.6509\n",
            "Epoch 353/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6444 - mae: 0.4571 - mse: 0.2269 - acc: 0.6255 - val_loss: 0.6266 - val_mae: 0.4473 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 354/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6447 - mae: 0.4576 - mse: 0.2271 - acc: 0.6219 - val_loss: 0.6274 - val_mae: 0.4479 - val_mse: 0.2187 - val_acc: 0.6522\n",
            "Epoch 355/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6417 - mae: 0.4562 - mse: 0.2257 - acc: 0.6277 - val_loss: 0.6269 - val_mae: 0.4475 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 356/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6432 - mae: 0.4566 - mse: 0.2263 - acc: 0.6182 - val_loss: 0.6263 - val_mae: 0.4470 - val_mse: 0.2182 - val_acc: 0.6551\n",
            "Epoch 357/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6463 - mae: 0.4584 - mse: 0.2278 - acc: 0.6138 - val_loss: 0.6269 - val_mae: 0.4475 - val_mse: 0.2185 - val_acc: 0.6539\n",
            "Epoch 358/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6432 - mae: 0.4568 - mse: 0.2264 - acc: 0.6219 - val_loss: 0.6270 - val_mae: 0.4476 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 359/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6462 - mae: 0.4579 - mse: 0.2278 - acc: 0.6141 - val_loss: 0.6274 - val_mae: 0.4480 - val_mse: 0.2188 - val_acc: 0.6514\n",
            "Epoch 360/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6431 - mae: 0.4566 - mse: 0.2262 - acc: 0.6199 - val_loss: 0.6278 - val_mae: 0.4482 - val_mse: 0.2189 - val_acc: 0.6522\n",
            "Epoch 361/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6424 - mae: 0.4567 - mse: 0.2260 - acc: 0.6197 - val_loss: 0.6270 - val_mae: 0.4476 - val_mse: 0.2186 - val_acc: 0.6539\n",
            "Epoch 362/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6451 - mae: 0.4577 - mse: 0.2273 - acc: 0.6197 - val_loss: 0.6271 - val_mae: 0.4477 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 363/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6466 - mae: 0.4583 - mse: 0.2279 - acc: 0.6194 - val_loss: 0.6274 - val_mae: 0.4478 - val_mse: 0.2187 - val_acc: 0.6530\n",
            "Epoch 364/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6446 - mae: 0.4576 - mse: 0.2269 - acc: 0.6214 - val_loss: 0.6268 - val_mae: 0.4474 - val_mse: 0.2185 - val_acc: 0.6555\n",
            "Epoch 365/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6455 - mae: 0.4573 - mse: 0.2274 - acc: 0.6184 - val_loss: 0.6271 - val_mae: 0.4476 - val_mse: 0.2186 - val_acc: 0.6522\n",
            "Epoch 366/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6445 - mae: 0.4572 - mse: 0.2269 - acc: 0.6262 - val_loss: 0.6278 - val_mae: 0.4482 - val_mse: 0.2189 - val_acc: 0.6514\n",
            "Epoch 367/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6442 - mae: 0.4572 - mse: 0.2268 - acc: 0.6196 - val_loss: 0.6271 - val_mae: 0.4476 - val_mse: 0.2186 - val_acc: 0.6534\n",
            "Epoch 368/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6432 - mae: 0.4565 - mse: 0.2264 - acc: 0.6211 - val_loss: 0.6272 - val_mae: 0.4477 - val_mse: 0.2187 - val_acc: 0.6534\n",
            "Epoch 369/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6426 - mae: 0.4565 - mse: 0.2262 - acc: 0.6235 - val_loss: 0.6260 - val_mae: 0.4468 - val_mse: 0.2181 - val_acc: 0.6564\n",
            "Epoch 370/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6471 - mae: 0.4582 - mse: 0.2280 - acc: 0.6189 - val_loss: 0.6270 - val_mae: 0.4476 - val_mse: 0.2186 - val_acc: 0.6547\n",
            "Epoch 371/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6441 - mae: 0.4572 - mse: 0.2268 - acc: 0.6213 - val_loss: 0.6279 - val_mae: 0.4483 - val_mse: 0.2190 - val_acc: 0.6505\n",
            "Epoch 372/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6423 - mae: 0.4562 - mse: 0.2259 - acc: 0.6221 - val_loss: 0.6264 - val_mae: 0.4471 - val_mse: 0.2183 - val_acc: 0.6568\n",
            "Epoch 373/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6413 - mae: 0.4558 - mse: 0.2255 - acc: 0.6247 - val_loss: 0.6270 - val_mae: 0.4475 - val_mse: 0.2186 - val_acc: 0.6547\n",
            "Epoch 374/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6435 - mae: 0.4564 - mse: 0.2266 - acc: 0.6116 - val_loss: 0.6280 - val_mae: 0.4483 - val_mse: 0.2190 - val_acc: 0.6514\n",
            "Epoch 375/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6423 - mae: 0.4561 - mse: 0.2260 - acc: 0.6182 - val_loss: 0.6275 - val_mae: 0.4479 - val_mse: 0.2188 - val_acc: 0.6514\n",
            "Epoch 376/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6403 - mae: 0.4556 - mse: 0.2251 - acc: 0.6270 - val_loss: 0.6279 - val_mae: 0.4482 - val_mse: 0.2190 - val_acc: 0.6514\n",
            "Epoch 377/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6445 - mae: 0.4575 - mse: 0.2269 - acc: 0.6219 - val_loss: 0.6277 - val_mae: 0.4480 - val_mse: 0.2189 - val_acc: 0.6509\n",
            "Epoch 378/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6428 - mae: 0.4561 - mse: 0.2263 - acc: 0.6173 - val_loss: 0.6270 - val_mae: 0.4475 - val_mse: 0.2186 - val_acc: 0.6551\n",
            "Epoch 379/500\n",
            "46/46 [==============================] - 10s 217ms/step - loss: 0.6446 - mae: 0.4571 - mse: 0.2270 - acc: 0.6117 - val_loss: 0.6280 - val_mae: 0.4483 - val_mse: 0.2190 - val_acc: 0.6501\n",
            "Epoch 380/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6429 - mae: 0.4563 - mse: 0.2261 - acc: 0.6243 - val_loss: 0.6276 - val_mae: 0.4479 - val_mse: 0.2188 - val_acc: 0.6509\n",
            "Epoch 381/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6439 - mae: 0.4571 - mse: 0.2267 - acc: 0.6173 - val_loss: 0.6273 - val_mae: 0.4477 - val_mse: 0.2187 - val_acc: 0.6518\n",
            "Epoch 382/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6412 - mae: 0.4556 - mse: 0.2255 - acc: 0.6219 - val_loss: 0.6264 - val_mae: 0.4471 - val_mse: 0.2183 - val_acc: 0.6559\n",
            "Epoch 383/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6448 - mae: 0.4571 - mse: 0.2272 - acc: 0.6133 - val_loss: 0.6279 - val_mae: 0.4482 - val_mse: 0.2190 - val_acc: 0.6497\n",
            "Epoch 384/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6467 - mae: 0.4581 - mse: 0.2280 - acc: 0.6187 - val_loss: 0.6272 - val_mae: 0.4477 - val_mse: 0.2187 - val_acc: 0.6522\n",
            "Epoch 385/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6449 - mae: 0.4574 - mse: 0.2271 - acc: 0.6241 - val_loss: 0.6271 - val_mae: 0.4476 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 386/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6437 - mae: 0.4568 - mse: 0.2266 - acc: 0.6211 - val_loss: 0.6273 - val_mae: 0.4478 - val_mse: 0.2187 - val_acc: 0.6518\n",
            "Epoch 387/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6439 - mae: 0.4571 - mse: 0.2266 - acc: 0.6211 - val_loss: 0.6274 - val_mae: 0.4479 - val_mse: 0.2188 - val_acc: 0.6514\n",
            "Epoch 388/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6423 - mae: 0.4558 - mse: 0.2260 - acc: 0.6267 - val_loss: 0.6269 - val_mae: 0.4475 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 389/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6453 - mae: 0.4577 - mse: 0.2274 - acc: 0.6160 - val_loss: 0.6273 - val_mae: 0.4478 - val_mse: 0.2187 - val_acc: 0.6514\n",
            "Epoch 390/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6435 - mae: 0.4563 - mse: 0.2265 - acc: 0.6187 - val_loss: 0.6270 - val_mae: 0.4475 - val_mse: 0.2186 - val_acc: 0.6534\n",
            "Epoch 391/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6430 - mae: 0.4562 - mse: 0.2262 - acc: 0.6190 - val_loss: 0.6271 - val_mae: 0.4476 - val_mse: 0.2186 - val_acc: 0.6526\n",
            "Epoch 392/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6434 - mae: 0.4567 - mse: 0.2264 - acc: 0.6259 - val_loss: 0.6267 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6543\n",
            "Epoch 393/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6427 - mae: 0.4564 - mse: 0.2262 - acc: 0.6228 - val_loss: 0.6275 - val_mae: 0.4478 - val_mse: 0.2188 - val_acc: 0.6522\n",
            "Epoch 394/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6443 - mae: 0.4570 - mse: 0.2269 - acc: 0.6221 - val_loss: 0.6285 - val_mae: 0.4486 - val_mse: 0.2193 - val_acc: 0.6489\n",
            "Epoch 395/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6449 - mae: 0.4571 - mse: 0.2272 - acc: 0.6173 - val_loss: 0.6269 - val_mae: 0.4474 - val_mse: 0.2185 - val_acc: 0.6539\n",
            "Epoch 396/500\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 0.6396 - mae: 0.4547 - mse: 0.2246 - acc: 0.6289 - val_loss: 0.6278 - val_mae: 0.4482 - val_mse: 0.2190 - val_acc: 0.6497\n",
            "Epoch 397/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6442 - mae: 0.4570 - mse: 0.2268 - acc: 0.6230 - val_loss: 0.6272 - val_mae: 0.4476 - val_mse: 0.2187 - val_acc: 0.6522\n",
            "Epoch 398/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6430 - mae: 0.4566 - mse: 0.2264 - acc: 0.6293 - val_loss: 0.6264 - val_mae: 0.4470 - val_mse: 0.2183 - val_acc: 0.6564\n",
            "Epoch 399/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6424 - mae: 0.4561 - mse: 0.2260 - acc: 0.6206 - val_loss: 0.6274 - val_mae: 0.4478 - val_mse: 0.2188 - val_acc: 0.6514\n",
            "Epoch 400/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6415 - mae: 0.4558 - mse: 0.2256 - acc: 0.6233 - val_loss: 0.6274 - val_mae: 0.4478 - val_mse: 0.2188 - val_acc: 0.6514\n",
            "Epoch 401/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6437 - mae: 0.4570 - mse: 0.2264 - acc: 0.6248 - val_loss: 0.6265 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6551\n",
            "Epoch 402/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6425 - mae: 0.4562 - mse: 0.2260 - acc: 0.6293 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6539\n",
            "Epoch 403/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6434 - mae: 0.4564 - mse: 0.2264 - acc: 0.6155 - val_loss: 0.6266 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6547\n",
            "Epoch 404/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6420 - mae: 0.4560 - mse: 0.2259 - acc: 0.6184 - val_loss: 0.6267 - val_mae: 0.4473 - val_mse: 0.2184 - val_acc: 0.6547\n",
            "Epoch 405/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6423 - mae: 0.4559 - mse: 0.2259 - acc: 0.6231 - val_loss: 0.6269 - val_mae: 0.4475 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 406/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6463 - mae: 0.4579 - mse: 0.2279 - acc: 0.6162 - val_loss: 0.6275 - val_mae: 0.4479 - val_mse: 0.2188 - val_acc: 0.6497\n",
            "Epoch 407/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6427 - mae: 0.4563 - mse: 0.2262 - acc: 0.6194 - val_loss: 0.6277 - val_mae: 0.4481 - val_mse: 0.2189 - val_acc: 0.6509\n",
            "Epoch 408/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6435 - mae: 0.4567 - mse: 0.2264 - acc: 0.6236 - val_loss: 0.6274 - val_mae: 0.4478 - val_mse: 0.2187 - val_acc: 0.6501\n",
            "Epoch 409/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6431 - mae: 0.4560 - mse: 0.2264 - acc: 0.6272 - val_loss: 0.6257 - val_mae: 0.4465 - val_mse: 0.2180 - val_acc: 0.6580\n",
            "Epoch 410/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6414 - mae: 0.4556 - mse: 0.2254 - acc: 0.6259 - val_loss: 0.6274 - val_mae: 0.4478 - val_mse: 0.2187 - val_acc: 0.6514\n",
            "Epoch 411/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6424 - mae: 0.4563 - mse: 0.2261 - acc: 0.6184 - val_loss: 0.6269 - val_mae: 0.4474 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 412/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6443 - mae: 0.4567 - mse: 0.2268 - acc: 0.6221 - val_loss: 0.6270 - val_mae: 0.4475 - val_mse: 0.2186 - val_acc: 0.6526\n",
            "Epoch 413/500\n",
            "46/46 [==============================] - 10s 217ms/step - loss: 0.6429 - mae: 0.4562 - mse: 0.2263 - acc: 0.6185 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6543\n",
            "Epoch 414/500\n",
            "46/46 [==============================] - 10s 209ms/step - loss: 0.6427 - mae: 0.4562 - mse: 0.2261 - acc: 0.6269 - val_loss: 0.6265 - val_mae: 0.4471 - val_mse: 0.2183 - val_acc: 0.6534\n",
            "Epoch 415/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6398 - mae: 0.4544 - mse: 0.2248 - acc: 0.6279 - val_loss: 0.6274 - val_mae: 0.4478 - val_mse: 0.2188 - val_acc: 0.6501\n",
            "Epoch 416/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6424 - mae: 0.4559 - mse: 0.2260 - acc: 0.6252 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 417/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6422 - mae: 0.4557 - mse: 0.2259 - acc: 0.6209 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 418/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6452 - mae: 0.4571 - mse: 0.2273 - acc: 0.6216 - val_loss: 0.6273 - val_mae: 0.4477 - val_mse: 0.2187 - val_acc: 0.6522\n",
            "Epoch 419/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6433 - mae: 0.4562 - mse: 0.2264 - acc: 0.6216 - val_loss: 0.6269 - val_mae: 0.4474 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 420/500\n",
            "46/46 [==============================] - 9s 207ms/step - loss: 0.6441 - mae: 0.4568 - mse: 0.2267 - acc: 0.6207 - val_loss: 0.6270 - val_mae: 0.4475 - val_mse: 0.2186 - val_acc: 0.6526\n",
            "Epoch 421/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6414 - mae: 0.4554 - mse: 0.2255 - acc: 0.6247 - val_loss: 0.6264 - val_mae: 0.4470 - val_mse: 0.2183 - val_acc: 0.6526\n",
            "Epoch 422/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6419 - mae: 0.4560 - mse: 0.2258 - acc: 0.6218 - val_loss: 0.6270 - val_mae: 0.4474 - val_mse: 0.2186 - val_acc: 0.6534\n",
            "Epoch 423/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6425 - mae: 0.4563 - mse: 0.2261 - acc: 0.6224 - val_loss: 0.6265 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 424/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6414 - mae: 0.4553 - mse: 0.2257 - acc: 0.6252 - val_loss: 0.6266 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 425/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6437 - mae: 0.4564 - mse: 0.2267 - acc: 0.6151 - val_loss: 0.6266 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 426/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6439 - mae: 0.4567 - mse: 0.2268 - acc: 0.6206 - val_loss: 0.6273 - val_mae: 0.4476 - val_mse: 0.2187 - val_acc: 0.6526\n",
            "Epoch 427/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6442 - mae: 0.4569 - mse: 0.2267 - acc: 0.6236 - val_loss: 0.6277 - val_mae: 0.4480 - val_mse: 0.2189 - val_acc: 0.6493\n",
            "Epoch 428/500\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 0.6409 - mae: 0.4556 - mse: 0.2253 - acc: 0.6277 - val_loss: 0.6273 - val_mae: 0.4477 - val_mse: 0.2187 - val_acc: 0.6526\n",
            "Epoch 429/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6449 - mae: 0.4574 - mse: 0.2271 - acc: 0.6192 - val_loss: 0.6271 - val_mae: 0.4476 - val_mse: 0.2186 - val_acc: 0.6526\n",
            "Epoch 430/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6415 - mae: 0.4555 - mse: 0.2255 - acc: 0.6214 - val_loss: 0.6263 - val_mae: 0.4469 - val_mse: 0.2183 - val_acc: 0.6543\n",
            "Epoch 431/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6437 - mae: 0.4566 - mse: 0.2266 - acc: 0.6131 - val_loss: 0.6275 - val_mae: 0.4479 - val_mse: 0.2188 - val_acc: 0.6501\n",
            "Epoch 432/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6449 - mae: 0.4572 - mse: 0.2272 - acc: 0.6141 - val_loss: 0.6273 - val_mae: 0.4477 - val_mse: 0.2187 - val_acc: 0.6501\n",
            "Epoch 433/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6445 - mae: 0.4574 - mse: 0.2270 - acc: 0.6238 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 434/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6439 - mae: 0.4568 - mse: 0.2266 - acc: 0.6209 - val_loss: 0.6269 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6539\n",
            "Epoch 435/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6434 - mae: 0.4563 - mse: 0.2264 - acc: 0.6201 - val_loss: 0.6266 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 436/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6413 - mae: 0.4557 - mse: 0.2255 - acc: 0.6226 - val_loss: 0.6263 - val_mae: 0.4469 - val_mse: 0.2183 - val_acc: 0.6551\n",
            "Epoch 437/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6438 - mae: 0.4569 - mse: 0.2267 - acc: 0.6226 - val_loss: 0.6272 - val_mae: 0.4476 - val_mse: 0.2187 - val_acc: 0.6526\n",
            "Epoch 438/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6415 - mae: 0.4554 - mse: 0.2255 - acc: 0.6213 - val_loss: 0.6270 - val_mae: 0.4474 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 439/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6417 - mae: 0.4557 - mse: 0.2256 - acc: 0.6276 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 440/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6421 - mae: 0.4559 - mse: 0.2259 - acc: 0.6179 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 441/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6432 - mae: 0.4563 - mse: 0.2264 - acc: 0.6224 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 442/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6434 - mae: 0.4561 - mse: 0.2263 - acc: 0.6267 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 443/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6413 - mae: 0.4554 - mse: 0.2255 - acc: 0.6245 - val_loss: 0.6268 - val_mae: 0.4472 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 444/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6420 - mae: 0.4559 - mse: 0.2259 - acc: 0.6204 - val_loss: 0.6270 - val_mae: 0.4474 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 445/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6441 - mae: 0.4571 - mse: 0.2268 - acc: 0.6207 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6539\n",
            "Epoch 446/500\n",
            "46/46 [==============================] - 10s 209ms/step - loss: 0.6424 - mae: 0.4561 - mse: 0.2261 - acc: 0.6240 - val_loss: 0.6269 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 447/500\n",
            "46/46 [==============================] - 10s 218ms/step - loss: 0.6448 - mae: 0.4571 - mse: 0.2272 - acc: 0.6172 - val_loss: 0.6265 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6543\n",
            "Epoch 448/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6426 - mae: 0.4558 - mse: 0.2261 - acc: 0.6192 - val_loss: 0.6269 - val_mae: 0.4474 - val_mse: 0.2186 - val_acc: 0.6522\n",
            "Epoch 449/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6433 - mae: 0.4564 - mse: 0.2265 - acc: 0.6182 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6530\n",
            "Epoch 450/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6420 - mae: 0.4556 - mse: 0.2258 - acc: 0.6240 - val_loss: 0.6269 - val_mae: 0.4474 - val_mse: 0.2186 - val_acc: 0.6526\n",
            "Epoch 451/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6418 - mae: 0.4558 - mse: 0.2257 - acc: 0.6236 - val_loss: 0.6269 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 452/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6410 - mae: 0.4553 - mse: 0.2254 - acc: 0.6248 - val_loss: 0.6274 - val_mae: 0.4477 - val_mse: 0.2188 - val_acc: 0.6505\n",
            "Epoch 453/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6440 - mae: 0.4569 - mse: 0.2267 - acc: 0.6247 - val_loss: 0.6270 - val_mae: 0.4474 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 454/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6426 - mae: 0.4558 - mse: 0.2262 - acc: 0.6204 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 455/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6426 - mae: 0.4557 - mse: 0.2261 - acc: 0.6206 - val_loss: 0.6269 - val_mae: 0.4474 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 456/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6443 - mae: 0.4568 - mse: 0.2269 - acc: 0.6223 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 457/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6440 - mae: 0.4563 - mse: 0.2267 - acc: 0.6213 - val_loss: 0.6265 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 458/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6434 - mae: 0.4562 - mse: 0.2264 - acc: 0.6204 - val_loss: 0.6269 - val_mae: 0.4473 - val_mse: 0.2186 - val_acc: 0.6526\n",
            "Epoch 459/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6459 - mae: 0.4574 - mse: 0.2276 - acc: 0.6163 - val_loss: 0.6269 - val_mae: 0.4474 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 460/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6427 - mae: 0.4562 - mse: 0.2262 - acc: 0.6179 - val_loss: 0.6270 - val_mae: 0.4474 - val_mse: 0.2186 - val_acc: 0.6530\n",
            "Epoch 461/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6422 - mae: 0.4561 - mse: 0.2259 - acc: 0.6241 - val_loss: 0.6263 - val_mae: 0.4469 - val_mse: 0.2183 - val_acc: 0.6539\n",
            "Epoch 462/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6434 - mae: 0.4565 - mse: 0.2265 - acc: 0.6233 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 463/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6428 - mae: 0.4561 - mse: 0.2262 - acc: 0.6207 - val_loss: 0.6266 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 464/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6440 - mae: 0.4566 - mse: 0.2267 - acc: 0.6196 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 465/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6419 - mae: 0.4557 - mse: 0.2258 - acc: 0.6213 - val_loss: 0.6271 - val_mae: 0.4475 - val_mse: 0.2186 - val_acc: 0.6522\n",
            "Epoch 466/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6445 - mae: 0.4571 - mse: 0.2270 - acc: 0.6228 - val_loss: 0.6269 - val_mae: 0.4474 - val_mse: 0.2185 - val_acc: 0.6518\n",
            "Epoch 467/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6434 - mae: 0.4564 - mse: 0.2263 - acc: 0.6255 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 468/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6445 - mae: 0.4573 - mse: 0.2271 - acc: 0.6139 - val_loss: 0.6266 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 469/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6439 - mae: 0.4567 - mse: 0.2266 - acc: 0.6207 - val_loss: 0.6265 - val_mae: 0.4470 - val_mse: 0.2183 - val_acc: 0.6539\n",
            "Epoch 470/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6423 - mae: 0.4561 - mse: 0.2260 - acc: 0.6204 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 471/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6433 - mae: 0.4567 - mse: 0.2265 - acc: 0.6199 - val_loss: 0.6266 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 472/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6450 - mae: 0.4576 - mse: 0.2274 - acc: 0.6088 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 473/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6416 - mae: 0.4556 - mse: 0.2256 - acc: 0.6255 - val_loss: 0.6265 - val_mae: 0.4470 - val_mse: 0.2183 - val_acc: 0.6539\n",
            "Epoch 474/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6433 - mae: 0.4563 - mse: 0.2264 - acc: 0.6185 - val_loss: 0.6265 - val_mae: 0.4470 - val_mse: 0.2183 - val_acc: 0.6539\n",
            "Epoch 475/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6462 - mae: 0.4577 - mse: 0.2278 - acc: 0.6153 - val_loss: 0.6265 - val_mae: 0.4470 - val_mse: 0.2183 - val_acc: 0.6539\n",
            "Epoch 476/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6446 - mae: 0.4571 - mse: 0.2270 - acc: 0.6257 - val_loss: 0.6265 - val_mae: 0.4470 - val_mse: 0.2183 - val_acc: 0.6539\n",
            "Epoch 477/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6423 - mae: 0.4562 - mse: 0.2261 - acc: 0.6218 - val_loss: 0.6265 - val_mae: 0.4470 - val_mse: 0.2184 - val_acc: 0.6539\n",
            "Epoch 478/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6413 - mae: 0.4555 - mse: 0.2255 - acc: 0.6216 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 479/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6443 - mae: 0.4568 - mse: 0.2268 - acc: 0.6238 - val_loss: 0.6266 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6539\n",
            "Epoch 480/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6393 - mae: 0.4545 - mse: 0.2245 - acc: 0.6335 - val_loss: 0.6268 - val_mae: 0.4472 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 481/500\n",
            "46/46 [==============================] - 10s 219ms/step - loss: 0.6438 - mae: 0.4565 - mse: 0.2267 - acc: 0.6202 - val_loss: 0.6266 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6539\n",
            "Epoch 482/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6424 - mae: 0.4558 - mse: 0.2260 - acc: 0.6213 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 483/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6408 - mae: 0.4554 - mse: 0.2253 - acc: 0.6240 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 484/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6422 - mae: 0.4559 - mse: 0.2259 - acc: 0.6272 - val_loss: 0.6266 - val_mae: 0.4471 - val_mse: 0.2184 - val_acc: 0.6543\n",
            "Epoch 485/500\n",
            "46/46 [==============================] - 10s 208ms/step - loss: 0.6434 - mae: 0.4564 - mse: 0.2266 - acc: 0.6167 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6534\n",
            "Epoch 486/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6453 - mae: 0.4573 - mse: 0.2274 - acc: 0.6151 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 487/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6467 - mae: 0.4581 - mse: 0.2280 - acc: 0.6092 - val_loss: 0.6267 - val_mae: 0.4472 - val_mse: 0.2184 - val_acc: 0.6539\n",
            "Epoch 488/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6419 - mae: 0.4556 - mse: 0.2258 - acc: 0.6248 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 489/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6446 - mae: 0.4568 - mse: 0.2270 - acc: 0.6173 - val_loss: 0.6268 - val_mae: 0.4472 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 490/500\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 0.6432 - mae: 0.4564 - mse: 0.2263 - acc: 0.6214 - val_loss: 0.6268 - val_mae: 0.4472 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 491/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6433 - mae: 0.4564 - mse: 0.2264 - acc: 0.6165 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 492/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6448 - mae: 0.4570 - mse: 0.2270 - acc: 0.6231 - val_loss: 0.6269 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 493/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6423 - mae: 0.4558 - mse: 0.2259 - acc: 0.6184 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 494/500\n",
            "46/46 [==============================] - 10s 207ms/step - loss: 0.6416 - mae: 0.4556 - mse: 0.2256 - acc: 0.6289 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 495/500\n",
            "46/46 [==============================] - 10s 210ms/step - loss: 0.6418 - mae: 0.4556 - mse: 0.2256 - acc: 0.6286 - val_loss: 0.6269 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6534\n",
            "Epoch 496/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6448 - mae: 0.4571 - mse: 0.2271 - acc: 0.6218 - val_loss: 0.6269 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6530\n",
            "Epoch 497/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6427 - mae: 0.4559 - mse: 0.2262 - acc: 0.6211 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 498/500\n",
            "46/46 [==============================] - 9s 206ms/step - loss: 0.6420 - mae: 0.4560 - mse: 0.2260 - acc: 0.6167 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 499/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6446 - mae: 0.4573 - mse: 0.2271 - acc: 0.6192 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n",
            "Epoch 500/500\n",
            "46/46 [==============================] - 9s 205ms/step - loss: 0.6390 - mae: 0.4545 - mse: 0.2244 - acc: 0.6287 - val_loss: 0.6268 - val_mae: 0.4473 - val_mse: 0.2185 - val_acc: 0.6526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLYXKx_IQDh7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "8e8e9d70-a14f-440e-8e7e-a93fc2fe67b7"
      },
      "source": [
        "df = pd.DataFrame(history.history)\n",
        "display(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>mae</th>\n",
              "      <th>mse</th>\n",
              "      <th>acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_mae</th>\n",
              "      <th>val_mse</th>\n",
              "      <th>val_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.944531</td>\n",
              "      <td>0.513726</td>\n",
              "      <td>0.336729</td>\n",
              "      <td>0.450680</td>\n",
              "      <td>1.032817</td>\n",
              "      <td>0.544610</td>\n",
              "      <td>0.368305</td>\n",
              "      <td>0.349478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.940724</td>\n",
              "      <td>0.513751</td>\n",
              "      <td>0.335819</td>\n",
              "      <td>0.450680</td>\n",
              "      <td>1.025125</td>\n",
              "      <td>0.544481</td>\n",
              "      <td>0.366722</td>\n",
              "      <td>0.349478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.931441</td>\n",
              "      <td>0.513691</td>\n",
              "      <td>0.333674</td>\n",
              "      <td>0.450510</td>\n",
              "      <td>1.012036</td>\n",
              "      <td>0.544183</td>\n",
              "      <td>0.363944</td>\n",
              "      <td>0.349478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.921154</td>\n",
              "      <td>0.514249</td>\n",
              "      <td>0.331540</td>\n",
              "      <td>0.450170</td>\n",
              "      <td>0.994717</td>\n",
              "      <td>0.543531</td>\n",
              "      <td>0.360021</td>\n",
              "      <td>0.349478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.904934</td>\n",
              "      <td>0.513713</td>\n",
              "      <td>0.327399</td>\n",
              "      <td>0.450340</td>\n",
              "      <td>0.972542</td>\n",
              "      <td>0.542287</td>\n",
              "      <td>0.354608</td>\n",
              "      <td>0.349478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0.644839</td>\n",
              "      <td>0.457114</td>\n",
              "      <td>0.227077</td>\n",
              "      <td>0.621769</td>\n",
              "      <td>0.626855</td>\n",
              "      <td>0.447307</td>\n",
              "      <td>0.218518</td>\n",
              "      <td>0.653027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>0.642655</td>\n",
              "      <td>0.455929</td>\n",
              "      <td>0.226150</td>\n",
              "      <td>0.621088</td>\n",
              "      <td>0.626818</td>\n",
              "      <td>0.447280</td>\n",
              "      <td>0.218501</td>\n",
              "      <td>0.652610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0.642042</td>\n",
              "      <td>0.456040</td>\n",
              "      <td>0.225964</td>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.626837</td>\n",
              "      <td>0.447294</td>\n",
              "      <td>0.218510</td>\n",
              "      <td>0.652610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0.644595</td>\n",
              "      <td>0.457299</td>\n",
              "      <td>0.227098</td>\n",
              "      <td>0.619218</td>\n",
              "      <td>0.626835</td>\n",
              "      <td>0.447292</td>\n",
              "      <td>0.218509</td>\n",
              "      <td>0.652610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>0.638959</td>\n",
              "      <td>0.454481</td>\n",
              "      <td>0.224413</td>\n",
              "      <td>0.628742</td>\n",
              "      <td>0.626837</td>\n",
              "      <td>0.447295</td>\n",
              "      <td>0.218510</td>\n",
              "      <td>0.652610</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss       mae       mse  ...   val_mae   val_mse   val_acc\n",
              "0    0.944531  0.513726  0.336729  ...  0.544610  0.368305  0.349478\n",
              "1    0.940724  0.513751  0.335819  ...  0.544481  0.366722  0.349478\n",
              "2    0.931441  0.513691  0.333674  ...  0.544183  0.363944  0.349478\n",
              "3    0.921154  0.514249  0.331540  ...  0.543531  0.360021  0.349478\n",
              "4    0.904934  0.513713  0.327399  ...  0.542287  0.354608  0.349478\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "495  0.644839  0.457114  0.227077  ...  0.447307  0.218518  0.653027\n",
              "496  0.642655  0.455929  0.226150  ...  0.447280  0.218501  0.652610\n",
              "497  0.642042  0.456040  0.225964  ...  0.447294  0.218510  0.652610\n",
              "498  0.644595  0.457299  0.227098  ...  0.447292  0.218509  0.652610\n",
              "499  0.638959  0.454481  0.224413  ...  0.447295  0.218510  0.652610\n",
              "\n",
              "[500 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkati-50QLir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "0e69b215-d0e9-476a-da1a-5395929ac96d"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.models import load_model\n",
        "from keras_bert import get_custom_objects\n",
        "\n",
        "with strategy.scope():\n",
        "    model = _create_model(data['test_features'].shape, data['class_count'])\n",
        "    model.load_weights(model_filename)\n",
        "\n",
        "predicted_test_labels = model.predict([data['test_features'], data['test_segments']]).argmax(axis=1)\n",
        "numeric_test_labels = np.array(data['test_labels']).argmax(axis=1)\n",
        "\n",
        "report = classification_report(numeric_test_labels, predicted_test_labels, output_dict=True,target_names=data['index2label'].values())\n",
        "\n",
        "display(pd.DataFrame(report).T)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.715815</td>\n",
              "      <td>0.772786</td>\n",
              "      <td>0.743210</td>\n",
              "      <td>1558.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.503506</td>\n",
              "      <td>0.428913</td>\n",
              "      <td>0.463226</td>\n",
              "      <td>837.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.652610</td>\n",
              "      <td>0.652610</td>\n",
              "      <td>0.652610</td>\n",
              "      <td>0.65261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.609660</td>\n",
              "      <td>0.600849</td>\n",
              "      <td>0.603218</td>\n",
              "      <td>2395.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>0.641617</td>\n",
              "      <td>0.652610</td>\n",
              "      <td>0.645362</td>\n",
              "      <td>2395.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              precision    recall  f1-score     support\n",
              "0              0.715815  0.772786  0.743210  1558.00000\n",
              "1              0.503506  0.428913  0.463226   837.00000\n",
              "accuracy       0.652610  0.652610  0.652610     0.65261\n",
              "macro avg      0.609660  0.600849  0.603218  2395.00000\n",
              "weighted avg   0.641617  0.652610  0.645362  2395.00000"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX6r66z_3Tse",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "f59729d9-e816-49f0-d8ad-0cd85f43c343"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = ['loss', 'acc']  # 使用する評価関数を指定\n",
        " \n",
        "plt.figure(figsize=(10, 5))  # グラフを表示するスペースを用意\n",
        " \n",
        "for i in range(len(metrics)):\n",
        " \n",
        "    metric = metrics[i]\n",
        " \n",
        "    plt.subplot(1, 2, i+1)  # figureを1×2のスペースに分け、i+1番目のスペースを使う\n",
        "    plt.title(metric)  # グラフのタイトルを表示\n",
        "    \n",
        "    plt_train = history.history[metric]  # historyから訓練データの評価を取り出す\n",
        "    plt_test = history.history['val_' + metric]  # historyからテストデータの評価を取り出す\n",
        "    \n",
        "    plt.plot(plt_train, label='training')  # 訓練データの評価をグラフにプロット\n",
        "    plt.plot(plt_test, label='test')  # テストデータの評価をグラフにプロット\n",
        "    plt.legend()  # ラベルの表示\n",
        "    \n",
        "plt.show()  # グラフの表示"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}